{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/swlh/how-to-scrape-large-amounts-of-reddit-data-using-pushshift-1d33bde9286"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# //*** Imports and Load Data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "from pmaw import PushshiftAPI\n",
    "\n",
    "\n",
    "#//*** Use the whole window in the IPYNB editor\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#//*** Maximize columns and rows displayed by pandas\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "api = PushshiftAPI()\n",
    "\n",
    "import datetime as dt\n",
    "before = int(dt.datetime(2021,6,26,0,0).timestamp())\n",
    "after = int(dt.datetime(2020,12,1,0,0).timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Update a target subreddit with new data\n",
    "#//*** subreddit = subbreddit name\n",
    "#//*** Method = 'before' or 'after' indicating if the records to be retrieved are before or after the target_utc. Defaults to After\n",
    "#//*** limit is the number or records to grab\n",
    "def update_subreddit(subreddit,method,limit):\n",
    "    import time\n",
    "    filename = f\".\\\\data\\\\{subreddit}_comments.csv.zip\"\n",
    "    print(f\"Reading csv: {filename}\")\n",
    "    start_time = time.time()\n",
    "    update_df = pd.read_csv(filename, compression = 'zip')\n",
    "    \n",
    "    print(f\"csv loaded: {round(time.time()-start_time,2)}s\")\n",
    "    \n",
    "    print(f\"csv Record count: {len(update_df)}\")\n",
    "\n",
    "    #//*** If before, get the largest (latest) utc\n",
    "    if method == 'before':\n",
    "        #//*** Get the Before utc from the stored csv\n",
    "        before = update_df['created_utc'].min() \n",
    "        \n",
    "        print(f\"Getting {limit} records before {before} utc\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        #//*** Download comments\n",
    "        comments = api.search_comments(subreddit=subreddit, limit=limit, before=before)\n",
    "        \n",
    "        print(f\"Download Time: {round(time.time()-start_time,2)}s\")\n",
    "        \n",
    "    elif method == 'after':\n",
    "        after = update_df['created_utc'].max() \n",
    "        \n",
    "        print(f\"Getting {limit} records after {after} utc\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        #//*** Download comments\n",
    "        comments = api.search_comments(subreddit=subreddit, limit=limit, after=after)\n",
    "        print(f\"Download Time: {round(time.time()-start_time,2)}s\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Method needs to be 'before' or 'after': [{method}] is invalid\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    #//***************************************************************************\n",
    "    #//*** Download Complete\n",
    "    #//***************************************************************************\n",
    "\n",
    "    #//*** Convert comments to Dataframe\n",
    "    raw_df = pd.DataFrame(comments)\n",
    "    \n",
    "    #//*** Columns to keep\n",
    "    keep_cols = [\"score\",\"total_awards_received\",\"created_utc\",\"is_submitter\",\"author_fullname\",\"body\",\"id\",\"link_id\",\"parent_id\",\"stickied\",\"permalink\",\"retrieved_on\",\"subreddit\",\"subreddit_id\"]\n",
    "    \n",
    "    #//*** Not all columns appear. This usually happens with small samples used for testing.\n",
    "    #//*** Only use the keep_cols that are actually in the sample. The missing columns will be added during concat later\n",
    "    actual_cols = []\n",
    "    \n",
    "    #//*** Loop through each column we want to keep\n",
    "    for col in keep_cols:\n",
    "        #//*** Add col to actual_cols if it exists\n",
    "        if col in raw_df.columns:\n",
    "            actual_cols.append(col)\n",
    "\n",
    "    #//*** Keep the important columns\n",
    "    raw_df = raw_df[actual_cols]\n",
    "\n",
    "    print(f\"Checking For Duplicates - Length Before: {len(raw_df)}\")\n",
    "    \n",
    "    #//*** Hash the body, will use to check for duplicates\n",
    "    #//*** Hash the body using sha-256\n",
    "    #Sha256: Reference https://www.pythonpool.com/python-sha256/\n",
    "\n",
    "    raw_df['hash'] = raw_df['body'].apply(lambda x:hashlib.sha256(x.encode()).hexdigest())\n",
    "\n",
    "\n",
    "    # dropping Duplicates First. No sense in processing these\n",
    "    raw_df.drop_duplicates(subset =\"hash\",keep = False, inplace = True)\n",
    "    \n",
    "    print(f\"Checking For Duplicates - Length After: {len(raw_df)}\")\n",
    "\n",
    "    #print(\"Begin Cleaning\")\n",
    "\n",
    "    #//*** Clean text, tokenize and remove stop words\n",
    "    #raw_df['clean'] = remove_stop_words(tokenize_series(mr_clean_text(raw_df['body'],{\"remove_empty\":False})))\n",
    "    \n",
    "    #//*** encode the comments\n",
    "    #//*** Breaking this out into a separate function for readability and possible future flexibility\n",
    "    #raw_df = encode_comments(raw_df)\n",
    "    \n",
    "    #//*** Combining existing dataframe with raw_df\n",
    "    update_df = pd.concat([update_df,raw_df])\n",
    "    print(f\"Combined Dataframe Size:{len(update_df)}\")\n",
    "\n",
    "    # Check for Duplicates. \n",
    "    update_df.drop_duplicates(subset =\"hash\",keep = False, inplace = True)\n",
    "    print(f\"Dropping Duplicates - New Size:{len(update_df)}\")\n",
    "\n",
    "    #print(\"Replace NaN with Zeros\")\n",
    "    update_df = update_df.fillna(0)\n",
    "    \n",
    "    #//*** Sort the Dataframe by UTC date. This keeps the time series chronological. \n",
    "    #//*** No need to reindex, since index will be ignored at csv read/write\n",
    "    update_df = update_df.sort_values('created_utc')\n",
    "\n",
    "    #//*** Convert is Submitter,Stickied field to Boolean.\n",
    "    #//*** Some early values are Integers and Strings\n",
    "    update_df['is_submitter'] = update_df['is_submitter'].astype('bool')\n",
    "    update_df['stickied'] = update_df['stickied'].astype('bool')\n",
    "    update_df['author_fullname'] = update_df['author_fullname'].astype('str')\n",
    "    \n",
    "    print(f\"Writing {filename}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    update_df.to_csv(filename,compression=\"zip\",index=False)    \n",
    "    \n",
    "    print(f\"File Written: {round(time.time()-start_time,2)}s\")\n",
    "    \n",
    "    print(f\"update_subreddit() Complete: {len(update_df)} records\")\n",
    "    \n",
    "    del update_df\n",
    "    del raw_df\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading csv: .\\data\\wallstreetbets_comments.csv.zip\n",
      "csv loaded: 2.77s\n",
      "csv Record count: 399786\n",
      "Getting 100 records before 1368383569 utc\n",
      "Total:: Success Rate: 100.00% - Requests: 1 - Batches: 1 - Items Remaining: 0\n",
      "Download Time: 1.47s\n",
      "Checking For Duplicates - Length Before: 100\n",
      "Checking For Duplicates - Length After: 82\n",
      "Combined Dataframe Size:399868\n",
      "Dropping Duplicates - New Size:399868\n",
      "Writing .\\data\\wallstreetbets_comments.csv.zip\n",
      "File Written: 9.0s\n",
      "update_subreddit() Complete: 399868 records\n"
     ]
    }
   ],
   "source": [
    "#//*** Add 100k Comments to wallstreetbets\n",
    "update_subreddit(\"wallstreetbets\",\"before\",100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(raw_df['hash'].unique()))\n",
    "#print(len(raw_df.tail()))\n",
    "#raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Reference to manually read & write to Data Frame\n",
    "#filename = f\".\\\\data\\\\wallstreetbets_comments.csv.zip\"\n",
    "#update_df = pd.read_csv(filename, compression = 'zip')\n",
    "#print(len(update_df))\n",
    "#update_df\n",
    "#filename = f\".\\\\data\\\\wallstreetbets_comments_comments.csv\"\n",
    "#update_df.to_csv(filename, compression = 'zip',index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_df.to_csv(\"reddit_comments.csv.zip\",compression=\"zip\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//***************************************\n",
    "#//*** Apply Common Cleanup operations\n",
    "#//***************************************\n",
    "#//*** In anticpation that I'll be re-using text cleanup code. I'm adding some robustness to the function.\n",
    "#//*** Adding kwargs to disable features that default to true.\n",
    "#//*** Whether an action is skipped or executed is based on a boolean value stored in action_dict.\n",
    "#//*** Key values will default to true. If code needs to be defaulted to False, a default_false list can be added later\n",
    "#//*** All Boolean kwarg keya are stored in kwarg list. This speeds up the coding of the action_dict.\n",
    "#//*** As Kwargs are added \n",
    "def mr_clean_text(input_series, input_options={}):\n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "    \n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Add some data validation. I'm preparing this function for additional use. I'm checking if future users (ie future me)\n",
    "    #//*** may throw some garbage at this function. Experience has taught me to fail safely wherever possible.\n",
    "\n",
    "    #//*** All kwargs are listed here. These initialize TRUE by default.\n",
    "    key_list = [ \"lower\", \"newline\", \"html\", \"remove_empty\", \"punctuation\" ]\n",
    "    \n",
    "    #//*** Build Action Dictionary\n",
    "    action_dict = { } \n",
    "    \n",
    "    #//*** Build the keys from kwarg_list and default them to TRUE\n",
    "    for key in key_list:\n",
    "        action_dict[key] = True\n",
    "        \n",
    "    #//*** Loop through the input kwargs (if any). Assign the action_dict values based on the kwargs:\n",
    "    for key,value in input_options.items():\n",
    "        print(key,value)\n",
    "        action_dict[key] = value\n",
    "    \n",
    "    \n",
    "    #//*************************************************************************\n",
    "    #//*** The Cleanup/Processing code is a straight lift from DSC550 - Week02\n",
    "    #//*************************************************************************\n",
    "    #//*** Convert to Lower Case, Default to True\n",
    "    if action_dict[\"lower\"]:\n",
    "        input_series = input_series.str.lower()\n",
    "    \n",
    "   \n",
    "    #//*** Remove New Lines\n",
    "    if action_dict[\"newline\"]:\n",
    "        #//*** Rmove \\r\\n\n",
    "        input_series = input_series.str.replace(r'\\r?\\n',\"\")\n",
    "\n",
    "        #//*** Remove \\n new lines\n",
    "        input_series = input_series.str.replace(r'\\n',\"\")\n",
    "\n",
    "    #//*** Remove html entities, observed entities are &gt; and &lt;. All HTML entities begin with & and end with ;.\n",
    "    #//*** Let's use regex to remove html entities\n",
    "    if action_dict[\"html\"]:\n",
    "        input_series = input_series.str.replace(r'&.*;',\"\")\n",
    "\n",
    "    #//*** Remove the empty lines\n",
    "    if action_dict[\"remove_empty\"]:\n",
    "        input_series = input_series[ input_series.str.len() > 0]\n",
    "\n",
    "    #//*** Remove punctuation\n",
    "    if action_dict[\"punctuation\"]:\n",
    "        #//*** Load libraries for punctuation if not already loaded.\n",
    "        #//*** Wrapping these in a try, no sense in importing libraries that already exist.\n",
    "        #//*** Unsure of the cost of reimporting libraries (if any). But testing if library is already loaded feels\n",
    "        #//*** like a good practice\n",
    "        try:\n",
    "            type(sys)\n",
    "        except:\n",
    "            import sys\n",
    "\n",
    "        try:\n",
    "            type(unicodedata)\n",
    "        except:\n",
    "            import unicodedata\n",
    "        \n",
    "        #//*** replace Comma and Period with a space.\n",
    "        for punct in [\",\",\".\",\"$\"]:\n",
    "            input_series = input_series.str.replace(punct,\" \")\n",
    "\n",
    "        #//*** Remove punctuation using the example from the book\n",
    "        punctuation = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P') )\n",
    "        input_series = input_series.str.translate(punctuation)\n",
    "\n",
    "    print(f\"Text Cleaning Time: {time.time() - start_time}\")\n",
    "\n",
    "    return input_series\n",
    "#//*** Remove Stop words from the input list\n",
    "def remove_stop_words(input_series):\n",
    "    \n",
    "    #//*** This function removes stop_words from a series.\n",
    "    #//*** Works with series.apply()\n",
    "    def apply_stop_words(input_list):\n",
    "\n",
    "        #//*** Load Stopwords   \n",
    "        for word in input_list:\n",
    "            if word in stop_words:\n",
    "                input_list.remove(word)\n",
    "        return input_list\n",
    "\n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "        \n",
    "    stopwords = nltk.corpus.stopwords\n",
    "\n",
    "    #//*** Stopwords requires an additional download\n",
    "    try:\n",
    "        type(stopwords)\n",
    "    except:\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "\n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    #//*** The stop_words include punctuation. Stop Word Contractions will not be filtered out.\n",
    "    stop_words = []\n",
    "\n",
    "    #//*** Remove apostrophies from the stop_words\n",
    "    for stop in stopwords.words('english'):\n",
    "        stop_words.append(stop.replace(\"'\",\"\"))\n",
    "\n",
    "    \n",
    "    #//*** Remove Stop words from the tokenized strings in the 'process' column\n",
    "    #input_series = input_series.apply(remove_stop_words,stop_words)\n",
    "    \n",
    "    input_series = input_series.apply(apply_stop_words)\n",
    "\n",
    "    print(f\"Stop Words Time: {time.time() - start_time}\")\n",
    "    \n",
    "    return input_series\n",
    "#//*** Tokenize a Series containing Strings.\n",
    "#//*** Breaking this out into it's own function for later reuse.\n",
    "#//*** Not a lot of code here, but it helps to keep the libraries localized. This creates standarization for future\n",
    "#//*** Stoneburner projects. Also has the ability to add functionality as needed.\n",
    "\n",
    "def tokenize_series(input_series):\n",
    "    \n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "    \n",
    "    word_tokenize = nltk.tokenize.word_tokenize \n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "    \n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    input_series = input_series.apply(word_tokenize)\n",
    "    \n",
    "    print(f\"Tokenize Time: {time.time() - start_time}\")\n",
    "    \n",
    "    return input_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Encodes the dataframe with a count of Ticker symbols in each comment.\n",
    "#//*** Called from update_subreddit(). This is broken out since we will likely need to adjust encoding parameters\n",
    "def encode_comments(raw_df):\n",
    "    import time\n",
    "    \n",
    "    print(\"Begin dataframe ticker symbol coding\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Build list of nasdaq and NYSE ticker symbols\n",
    "    #//*** Reads from Excel file.\n",
    "    #//*** Gets the Symbol column, and converts to lower case, \n",
    "    nyse = pd.read_csv(\"NYSE_20210625.csv\",header=None)[0].str.lower()\n",
    "    nasdaq = pd.read_csv(\"NASDAQ_20210625.csv\",header=None)[0].str.lower()\n",
    "\n",
    "    #//*** Removes symbols with 1 and 2 character listings\n",
    "    nyse = list(nyse[nyse.apply(lambda x: len(x)>2) ])\n",
    "    nasdaq = list(nasdaq[nasdaq.apply(lambda x: len(x)>2) ])\n",
    "\n",
    "    #//*** Combines both lists\n",
    "    symbols = nyse + nasdaq\n",
    "    \n",
    "\n",
    "    #//*** Count each Stock mention add it to a dictionary of lists. Each list is filled with 0s. The Specific row index is updated with the relevant count. \n",
    "    #//*** This Generates a word count matrix\n",
    "    stock_dict = {}\n",
    "\n",
    "    #//*** Keep Track of Rows\n",
    "    index = 0\n",
    "\n",
    "    for row in raw_df.iterrows():\n",
    "\n",
    "        #//*** Get the cleaned body text\n",
    "        body = row[1]['clean']\n",
    "\n",
    "        #//*** For Each Stock Symbol\n",
    "        for stock in symbols:\n",
    "\n",
    "            #//*** Check if Stock exists in Body\n",
    "            if stock in body:\n",
    "\n",
    "                #//*** Reset the stock counter\n",
    "                count = 0\n",
    "\n",
    "                #//*** Loop through body and county ticker mentions\n",
    "                for word in body:\n",
    "                    #//*** If word found increment count\n",
    "                    if stock == word:\n",
    "                        count += 1\n",
    "\n",
    "                #//*** Check if symbol is in stock_dict\n",
    "                if stock not in stock_dict.keys():    \n",
    "\n",
    "                    #//*** If not, then build it\n",
    "                    stock_dict[stock] = np.zeros(len(raw_df))\n",
    "\n",
    "                #//*** Update the stock value at the \n",
    "                stock_dict[stock][index] = count\n",
    "\n",
    "        #//*** Increment Index to keep with row index\n",
    "        index +=1   \n",
    "\n",
    "    #//*** Loop through the dictionary key and lists\n",
    "    for col,values in stock_dict.items():\n",
    "\n",
    "        #//*** Add each key (which is a stock ticker symbol) as a column using the list of ticker counts for Data\n",
    "        raw_df[col] = values.astype('int')\n",
    "\n",
    "    print(f\"Encoding Time: {round(time.time()-start_time,2)}s\")\n",
    "    \n",
    "    return raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Push shift scraper reference\n",
    "\"\"\"\n",
    "#//*** Download the First 100,000 Comments from reddit pushsift\n",
    "subreddit=\"wallstreetbets\"\n",
    "limit=100\n",
    "#comments = api.search_comments(subreddit=subreddit, limit=limit, before=before after=after)\n",
    "comments = api.search_comments(subreddit=subreddit, limit=limit, after=after)\n",
    "print(f'Retrieved {len(comments)} comments from Pushshift')\n",
    "\n",
    "#//*** Convert comments to Dataframe\n",
    "comments_df = pd.DataFrame(comments)\n",
    "\n",
    "#//*** Save DataFrame to a file for processing\n",
    "comments_df.to_csv(f\"{subreddit}_raw_comments.csv.zip\",compression=\"zip\",index=False)\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Reference to create Stock ticker count matrix\n",
    "\"\"\"\n",
    "\n",
    "#//*** Build list of ticker symbols from NYSE and NASDAQ\n",
    "#//*** Reads from Excel file.\n",
    "#//*** Gets the Symbol column, and converts to lower case, \n",
    "nyse = pd.read_csv(\"NYSE_20210625.csv\",header=None)[0].str.lower()\n",
    "nasdaq = pd.read_csv(\"NASDAQ_20210625.csv\",header=None)[0].str.lower()\n",
    "\n",
    "#//*** Removes symbols with 1 and 2 character listings\n",
    "nyse = list(nyse[nyse.apply(lambda x: len(x)>2) ])\n",
    "nasdaq = list(nasdaq[nasdaq.apply(lambda x: len(x)>2) ])\n",
    "\n",
    "symbols = nyse + nasdaq\n",
    "\n",
    "#//*** Count each Stock mention add it to a dictionary of lists. Each list is filled with 0s. The Specific row index is updated with the relevant count. \n",
    "#//*** This Generates a word count matrix\n",
    "stock_dict = {}\n",
    "\n",
    "#//*** Keep Track of Rows\n",
    "index = 0\n",
    "\n",
    "for row in raw_df.iterrows():\n",
    "    \n",
    "    #//*** Get the cleaned body text\n",
    "    body = row[1]['clean']\n",
    "    \n",
    "    #//*** For Each Stock Symbol\n",
    "    for stock in symbols:\n",
    "        \n",
    "        #//*** Check if Stock exists in Body\n",
    "        if stock in body:\n",
    "            \n",
    "            #//*** Reset the stock counter\n",
    "            count = 0\n",
    "            \n",
    "            #//*** Loop through body and county ticker mentions\n",
    "            for word in body:\n",
    "                #//*** If word found increment count\n",
    "                if stock == word:\n",
    "                    count += 1\n",
    "                    \n",
    "            #//*** Check if symbol is in stock_dict\n",
    "            if stock not in stock_dict.keys():    \n",
    "\n",
    "                #//*** If not, then build it\n",
    "                stock_dict[stock] = np.zeros(len(raw_df))\n",
    "            \n",
    "            #//*** Update the stock value at the \n",
    "            stock_dict[stock][index] = count\n",
    "\n",
    "    #//*** Increment Index to keep with row index\n",
    "    index +=1   \n",
    "\n",
    "#//*** Loop through the dictionary key and lists\n",
    "for col,values in stock_dict.items():\n",
    "    \n",
    "    #//*** Add each key (which is a stock ticker symbol) as a column using the list of ticker counts for Data\n",
    "    raw_df[col] = values.astype('int')\n",
    "    \n",
    "\"\"\"\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
