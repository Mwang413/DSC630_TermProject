{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/swlh/how-to-scrape-large-amounts-of-reddit-data-using-pushshift-1d33bde9286"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# //*** Imports and Load Data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "from pmaw import PushshiftAPI\n",
    "\n",
    "\n",
    "#//*** Use the whole window in the IPYNB editor\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#//*** Maximize columns and rows displayed by pandas\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "api = PushshiftAPI()\n",
    "\n",
    "import datetime as dt\n",
    "before = int(dt.datetime(2021,6,26,0,0).timestamp())\n",
    "after = int(dt.datetime(2020,12,1,0,0).timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Update a target subreddit with new data\n",
    "#//*** subreddit = subbreddit name\n",
    "#//*** Method = 'before' or 'after' indicating if the records to be retrieved are before or after the target_utc. Defaults to After\n",
    "#//*** limit is the number or records to grab\n",
    "def update_subreddit(subreddit,method,limit):\n",
    "    import time\n",
    "    filename = f\".\\\\data\\\\{subreddit}_comments.csv.zip\"\n",
    "    print(f\"Reading csv: {filename}\")\n",
    "    start_time = time.time()\n",
    "    update_df = pd.read_csv(filename, compression = 'zip')\n",
    "    \n",
    "    print(f\"csv loaded: {round(time.time()-start_time,2)}s\")\n",
    "    \n",
    "    print(f\"csv Record count: {len(update_df)}\")\n",
    "\n",
    "    #//*** If before, get the largest (latest) utc\n",
    "    if method == 'before':\n",
    "        #//*** Get the Before utc from the stored csv\n",
    "        before = update_df['created_utc'].min() \n",
    "        \n",
    "        print(f\"Getting {limit} records before {before} utc\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        #//*** Download comments\n",
    "        comments = api.search_comments(subreddit=subreddit, limit=limit, before=before)\n",
    "        \n",
    "        print(f\"Download Time: {round(time.time()-start_time,2)}s\")\n",
    "        \n",
    "    elif method == 'after':\n",
    "        after = update_df['created_utc'].max() \n",
    "        \n",
    "        print(f\"Getting {limit} records after {after} utc\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        #//*** Download comments\n",
    "        comments = api.search_comments(subreddit=subreddit, limit=limit, after=after)\n",
    "        print(f\"Download Time: {round(time.time()-start_time,2)}s\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Method needs to be 'before' or 'after': [{method}] is invalid\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    #//***************************************************************************\n",
    "    #//*** Download Complete\n",
    "    #//***************************************************************************\n",
    "\n",
    "    #//*** Convert comments to Dataframe\n",
    "    raw_df = pd.DataFrame(comments)\n",
    "    \n",
    "    #//*** Columns to keep\n",
    "    keep_cols = [\"score\",\"total_awards_received\",\"created_utc\",\"is_submitter\",\"author_fullname\",\"body\",\"id\",\"link_id\",\"parent_id\",\"stickied\",\"permalink\",\"retrieved_on\",\"subreddit\",\"subreddit_id\"]\n",
    "    \n",
    "    #//*** Not all columns appear. This usually happens with small samples used for testing.\n",
    "    #//*** Only use the keep_cols that are actually in the sample. The missing columns will be added during concat later\n",
    "    actual_cols = []\n",
    "    \n",
    "    #//*** Loop through each column we want to keep\n",
    "    for col in keep_cols:\n",
    "        #//*** Add col to actual_cols if it exists\n",
    "        if col in raw_df.columns:\n",
    "            actual_cols.append(col)\n",
    "\n",
    "    #//*** Keep the important columns\n",
    "    raw_df = raw_df[actual_cols]\n",
    "\n",
    "    print(f\"Checking For Duplicates - Length Before: {len(raw_df)}\")\n",
    "    \n",
    "    #//*** Hash the body, will use to check for duplicates\n",
    "    #//*** Hash the body using sha-256\n",
    "    #Sha256: Reference https://www.pythonpool.com/python-sha256/\n",
    "\n",
    "    raw_df['hash'] = raw_df['body'].apply(lambda x:hashlib.sha256(x.encode()).hexdigest())\n",
    "\n",
    "\n",
    "    # dropping Duplicates First. No sense in processing these\n",
    "    raw_df.drop_duplicates(subset =\"hash\",keep = False, inplace = True)\n",
    "    \n",
    "    print(f\"Checking For Duplicates - Length After: {len(raw_df)}\")\n",
    "\n",
    "    #print(\"Begin Cleaning\")\n",
    "\n",
    "    #//*** Clean text, tokenize and remove stop words\n",
    "    #raw_df['clean'] = remove_stop_words(tokenize_series(mr_clean_text(raw_df['body'],{\"remove_empty\":False})))\n",
    "    \n",
    "    #//*** encode the comments\n",
    "    #//*** Breaking this out into a separate function for readability and possible future flexibility\n",
    "    #raw_df = encode_comments(raw_df)\n",
    "    \n",
    "    #//*** Combining existing dataframe with raw_df\n",
    "    update_df = pd.concat([update_df,raw_df])\n",
    "    print(f\"Combined Dataframe Size:{len(update_df)}\")\n",
    "\n",
    "    # Check for Duplicates. \n",
    "    update_df.drop_duplicates(subset =\"hash\",keep = False, inplace = True)\n",
    "    print(f\"Dropping Duplicates - New Size:{len(update_df)}\")\n",
    "\n",
    "    #print(\"Replace NaN with Zeros\")\n",
    "    update_df = update_df.fillna(0)\n",
    "    \n",
    "    #//*** Sort the Dataframe by UTC date. This keeps the time series chronological. \n",
    "    #//*** No need to reindex, since index will be ignored at csv read/write\n",
    "    update_df = update_df.sort_values('created_utc')\n",
    "\n",
    "    #//*** Convert is Submitter,Stickied field to Boolean.\n",
    "    #//*** Some early values are Integers and Strings\n",
    "    update_df['is_submitter'] = update_df['is_submitter'].astype('bool')\n",
    "    update_df['stickied'] = update_df['stickied'].astype('bool')\n",
    "    update_df['author_fullname'] = update_df['author_fullname'].astype('str')\n",
    "    \n",
    "    print(f\"Writing {filename}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    update_df.to_csv(filename,compression=\"zip\",index=False)    \n",
    "    \n",
    "    print(f\"File Written: {round(time.time()-start_time,2)}s\")\n",
    "    \n",
    "    print(f\"update_subreddit() Complete: {len(update_df)} records\")\n",
    "    \n",
    "    del update_df\n",
    "    del raw_df\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading csv: .\\data\\wallstreetbets_comments.csv.zip\n",
      "csv loaded: 2.77s\n",
      "csv Record count: 399786\n",
      "Getting 100 records before 1368383569 utc\n",
      "Total:: Success Rate: 100.00% - Requests: 1 - Batches: 1 - Items Remaining: 0\n",
      "Download Time: 1.47s\n",
      "Checking For Duplicates - Length Before: 100\n",
      "Checking For Duplicates - Length After: 82\n",
      "Combined Dataframe Size:399868\n",
      "Dropping Duplicates - New Size:399868\n",
      "Writing .\\data\\wallstreetbets_comments.csv.zip\n",
      "File Written: 9.0s\n",
      "update_subreddit() Complete: 399868 records\n"
     ]
    }
   ],
   "source": [
    "#//*** Add 100k Comments to wallstreetbets\n",
    "update_subreddit(\"wallstreetbets\",\"before\",100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(raw_df['hash'].unique()))\n",
    "#print(len(raw_df.tail()))\n",
    "#raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Reference to manually read & write to Data Frame\n",
    "#filename = f\".\\\\data\\\\wallstreetbets_comments.csv.zip\"\n",
    "#update_df = pd.read_csv(filename, compression = 'zip')\n",
    "#print(len(update_df))\n",
    "#update_df\n",
    "#filename = f\".\\\\data\\\\wallstreetbets_comments_comments.csv\"\n",
    "#update_df.to_csv(filename, compression = 'zip',index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_df.to_csv(\"reddit_comments.csv.zip\",compression=\"zip\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Push shift scraper reference\n",
    "\"\"\"\n",
    "#//*** Download the First 100,000 Comments from reddit pushsift\n",
    "subreddit=\"wallstreetbets\"\n",
    "limit=100\n",
    "#comments = api.search_comments(subreddit=subreddit, limit=limit, before=before after=after)\n",
    "comments = api.search_comments(subreddit=subreddit, limit=limit, after=after)\n",
    "print(f'Retrieved {len(comments)} comments from Pushshift')\n",
    "\n",
    "#//*** Convert comments to Dataframe\n",
    "comments_df = pd.DataFrame(comments)\n",
    "\n",
    "#//*** Save DataFrame to a file for processing\n",
    "comments_df.to_csv(f\"{subreddit}_raw_comments.csv.zip\",compression=\"zip\",index=False)\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Reference to create Stock ticker count matrix\n",
    "\"\"\"\n",
    "\n",
    "#//*** Build list of ticker symbols from NYSE and NASDAQ\n",
    "#//*** Reads from Excel file.\n",
    "#//*** Gets the Symbol column, and converts to lower case, \n",
    "nyse = pd.read_csv(\"NYSE_20210625.csv\",header=None)[0].str.lower()\n",
    "nasdaq = pd.read_csv(\"NASDAQ_20210625.csv\",header=None)[0].str.lower()\n",
    "\n",
    "#//*** Removes symbols with 1 and 2 character listings\n",
    "nyse = list(nyse[nyse.apply(lambda x: len(x)>2) ])\n",
    "nasdaq = list(nasdaq[nasdaq.apply(lambda x: len(x)>2) ])\n",
    "\n",
    "symbols = nyse + nasdaq\n",
    "\n",
    "#//*** Count each Stock mention add it to a dictionary of lists. Each list is filled with 0s. The Specific row index is updated with the relevant count. \n",
    "#//*** This Generates a word count matrix\n",
    "stock_dict = {}\n",
    "\n",
    "#//*** Keep Track of Rows\n",
    "index = 0\n",
    "\n",
    "for row in raw_df.iterrows():\n",
    "    \n",
    "    #//*** Get the cleaned body text\n",
    "    body = row[1]['clean']\n",
    "    \n",
    "    #//*** For Each Stock Symbol\n",
    "    for stock in symbols:\n",
    "        \n",
    "        #//*** Check if Stock exists in Body\n",
    "        if stock in body:\n",
    "            \n",
    "            #//*** Reset the stock counter\n",
    "            count = 0\n",
    "            \n",
    "            #//*** Loop through body and county ticker mentions\n",
    "            for word in body:\n",
    "                #//*** If word found increment count\n",
    "                if stock == word:\n",
    "                    count += 1\n",
    "                    \n",
    "            #//*** Check if symbol is in stock_dict\n",
    "            if stock not in stock_dict.keys():    \n",
    "\n",
    "                #//*** If not, then build it\n",
    "                stock_dict[stock] = np.zeros(len(raw_df))\n",
    "            \n",
    "            #//*** Update the stock value at the \n",
    "            stock_dict[stock][index] = count\n",
    "\n",
    "    #//*** Increment Index to keep with row index\n",
    "    index +=1   \n",
    "\n",
    "#//*** Loop through the dictionary key and lists\n",
    "for col,values in stock_dict.items():\n",
    "    \n",
    "    #//*** Add each key (which is a stock ticker symbol) as a column using the list of ticker counts for Data\n",
    "    raw_df[col] = values.astype('int')\n",
    "    \n",
    "\"\"\"\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
