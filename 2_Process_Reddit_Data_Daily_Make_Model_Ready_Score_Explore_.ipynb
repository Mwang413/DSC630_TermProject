{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import platform\n",
    "\n",
    "import stoneburner\n",
    "#//*** Custom Functions:\n",
    "#//*** mr_clean_text(input_series)\n",
    "#//*** tokenize_series(input_series)\n",
    "#//*** remove_stop_words(input_series)\n",
    "\n",
    "# //*** Imports and Load Data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#//*** Use the whole window in the IPYNB editor\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "subreddits = [\"wallstreetbets\", \"stocks\", \"wallstreetbetsOGs\", \"spacs\", \"investing\", \"pennystocks\", \"stockmarket\", \"options\", \"robinhoodpennystocks\", \"wallstreetbetsnew\", \"smallstreetbets\"]\n",
    "filepath = \"./data/\"\n",
    "filename_suffix = \"_comments.csv.zip\"\n",
    "#//*** Maximize columns and rows displayed by pandas\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_comments(subreddit,filepath,filename_suffix,keep_cols=['created_utc','score','total_awards_received','is_submitter','author_fullname','body']):\n",
    "\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    #//*** Initialize Output dataframe\n",
    "    output_df = pd.DataFrame()\n",
    "\n",
    "    #//*** Load each Subreddit for Aggregation\n",
    "    for subreddit in subreddits:\n",
    "        #//*** Filepath + subreddit name + csv.zip\n",
    "        input_filename = filepath+subreddit+filename_suffix\n",
    "\n",
    "        print(f\"Reading Compressed CSV: {input_filename}\")\n",
    "\n",
    "        #//*** Read Each DataFrame and combine with output_df\n",
    "        output_df = pd.concat([output_df,pd.read_csv(input_filename,compression='zip' )])\n",
    "\n",
    "    #//*** All Files read\n",
    "    #//*** Reset the output_df index, since multiple indexes have been combined\n",
    "    output_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(f\"Files Loaded: {round(time.time()-start_time,2)}s\")\n",
    "    print(f\"Total Records: {len(output_df)}\")\n",
    "    \n",
    "    #//*** Ensure the body column is a string\n",
    "    output_df['body'] = output_df['body'].astype('str')\n",
    "\n",
    "    #//*** Convert UTC to date (not datetime)\n",
    "    #//** Second pass goes from 12-21 to 4-19\n",
    "    try:\n",
    "        output_df['created_utc'] = output_df['created_utc'].apply(lambda x: datetime.fromtimestamp(x))\n",
    "    except:\n",
    "        print()\n",
    "\n",
    "    try:\n",
    "        #//*** Keep just date and body fields\n",
    "        output_df = output_df[keep_cols]\n",
    "    except:\n",
    "        print(\"Skipping Keep Cols\")\n",
    "        \n",
    "    return output_df\n",
    "\n",
    "def build_stocks(symbol,interval,targets):\n",
    "    \n",
    "    stock_df = pd.read_csv(f\"./stocks/{symbol}_{interval}.csv.zip\")\n",
    "    stock_df\n",
    "    \n",
    "    if 'date' in stock_df.columns:\n",
    "        stock_df['date'] = pd.to_datetime(stock_df['date'])\n",
    "        stock_df = stock_df.rename(columns={'date':'time'})\n",
    "    else:\n",
    "        stock_df['time'] = pd.to_datetime(stock_df['time'])\n",
    "    \n",
    "    offset_targets = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "    for offset in targets:\n",
    "        #//*** create a list of nan values of x length\n",
    "        nan_list = list(np.empty( offset )* np.nan )\n",
    "\n",
    "        #//*** Create target variable Price which is stocks + x columns in advance\n",
    "        #//*** Takes the closing price starting at x and gets the remainder, this generates the offset\n",
    "        #//*** nan_list fills the missing x values with nans\n",
    "        stock_df[f'target_{offset}'] = list(stock_df['close'][offset:]) + nan_list \n",
    "\n",
    "    stock_df = stock_df[:offset*-1]\n",
    "    \n",
    "    #//*** Keeping this cool chunk of code as a reference\n",
    "    #stock_df['time'] = stock_df['time'].apply(lambda x: x.timestamp())\n",
    "    \n",
    "    #//*** Remove Comments older than the first stock price\n",
    "    #raw_df = raw_df[ raw_df['created_utc'] >= stock_df['time'].min() ]\n",
    "    \n",
    "    #//*** Remove Stock Prices older than the first Comments price\n",
    "    #stock_df = stock_df[ stock_df['time'] >= raw_df['created_utc'].min() ]\n",
    "    \n",
    "\n",
    "    \n",
    "    #//*** Reorder Comments by date\n",
    "    raw_df.sort_values('created_utc',inplace=True,ignore_index=True)\n",
    "    \n",
    "    #//*** Reorder Stocks by date\n",
    "    stock_df.sort_values('time',inplace=True, ignore_index=True)\n",
    "    \n",
    "    return stock_df\n",
    "  \n",
    "def group_dataframe_by_time(stock_df,cdf,symbol=None):\n",
    "    groups = stock_df.groupby('time')\n",
    "\n",
    "    #https://www.geeksforgeeks.org/how-to-iterate-over-dataframe-groups-in-python-pandas/\n",
    "    key_list = list(groups.groups.keys())\n",
    "\n",
    "    out_df = pd.DataFrame()\n",
    "\n",
    "    start_time = time.time()\n",
    "    comment_min_time = raw_df['created_utc'].min()\n",
    "    comment_max_time = raw_df['created_utc'].max()\n",
    "    min_index = 0\n",
    "    start_time = time.time()\n",
    "    print(\"Processing...\")\n",
    "    for x in range(len(key_list)-1):\n",
    "\n",
    "        #//*** Get a single line of stocks as a dataframe\n",
    "        loop_stocks_df = groups.get_group((key_list)[x]).copy()  \n",
    "\n",
    "        t1 = groups.get_group((key_list)[x])['time'].iloc[0]\n",
    "        t2 = groups.get_group((key_list)[x+1])['time'].iloc[0]\n",
    "\n",
    "        #search through cdf to find comments that are between t1 and t2\n",
    "        #if len(cdf[ (cdf['created_utc'] > t1) & (cdf['created_utc'] < t2) ]) > 0:\n",
    "        #temp_df = cdf.iloc[min_index:]\n",
    "        temp_df = cdf[(cdf['created_utc'] >= t1) & (cdf['created_utc'] < t2) ]\n",
    "\n",
    "        if len(temp_df) == 0:\n",
    "                #//*** No COmments on this Date\n",
    "                loop_stocks_df['body'] = \" \"\n",
    "                loop_stocks_df['comment_count'] = 0\n",
    "                loop_stocks_df['is_submitter'] = 0\n",
    "                loop_stocks_df['score'] = 0 \n",
    "                loop_stocks_df['total_awards_received'] = 0 \n",
    "                loop_stocks_df['author_fullname'] = pd.Series(dtype=object)\n",
    "        else:\n",
    "            #print(temp_df.index[0])\n",
    "            #print(temp_df.index[-1])\n",
    "            #print(cdf.iloc[temp_df.index[0]:temp_df.index[-1]])\n",
    "            temp_df = cdf.iloc[temp_df.index[0]:temp_df.index[-1]]\n",
    "            #//*** Get all the body comments and combine them\n",
    "            \n",
    "            #//*** If symbol is NOT specified take all the comments\n",
    "            if symbol == None:\n",
    "                loop_stocks_df['body'] = \" \".join(list(temp_df['body']))\n",
    "            else:\n",
    "                #//*** Symbol specified, only use comments that include the symbol\n",
    "                \n",
    "                if len(temp_df[temp_df['body'].str.contains(symbol)]) > 0:\n",
    "                    symbol_body_df = temp_df[temp_df['body'].str.contains(symbol)]\n",
    "                    #print(symbol_body_df)\n",
    "                    loop_stocks_df['body'] = \" \".join(list(symbol_body_df['body']))\n",
    "                    #print(loop_stocks_df)\n",
    "                    #if len(symbol_body_df[symbol_body_df['body'].str.find('symbol') > -1]):\n",
    "                    #    print(symbol_body_df[symbol_body_df['body'].str.find('symbol') > -1])\n",
    "                    #    break\n",
    "                \n",
    "            \n",
    "            \n",
    "\n",
    "            #//*** Get a comment count, BC IDK Y\n",
    "            loop_stocks_df['comment_count'] = len(temp_df['body']) \n",
    "            \n",
    "            loop_stocks_df['is_submitter'] = temp_df['is_submitter'].sum()\n",
    "            \n",
    "            loop_stocks_df['score'] = temp_df['score'].sum()\n",
    "            \n",
    "            loop_stocks_df['total_awards_received'] = temp_df['total_awards_received'].sum()\n",
    "            \n",
    "            loop_stocks_df['author_fullname'] = (temp_df['author_fullname'])\n",
    "        #//*** Secret to speeding up algorithm. Get the Index value of the last item found -1. \n",
    "        #//*** When Searching above, start the search from this index. Seems to speed things up. Since we're skipping past elements that we've already found\n",
    "        if len(temp_df) > 0: \n",
    "            min_index = temp_df.index[-1]\n",
    "            \n",
    "        #print(temp_df)\n",
    "        #print(temp_df.score.sum())\n",
    "        #print(loop_stocks_df)\n",
    "        #break\n",
    "\n",
    "        #//*** Add the single line of loop_stocks_df to bin_df    \n",
    "        out_df = pd.concat([out_df,loop_stocks_df])\n",
    "            #print(loop_df.index)\n",
    "            #print(loop_df)\n",
    "            #cdf = cdf.drop(index=loop_df.index)\n",
    "    print(f\"Merge Built: {round(time.time()-start_time,2)}s\")   \n",
    "    return out_df\n",
    "\n",
    "#model_df = group_dataframe_by_time(stock_df,raw_df,symbol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Compressed CSV: ./data/wallstreetbets_comments.csv.zip\n",
      "Reading Compressed CSV: ./data/stocks_comments.csv.zip\n",
      "Reading Compressed CSV: ./data/wallstreetbetsOGs_comments.csv.zip\n",
      "Reading Compressed CSV: ./data/spacs_comments.csv.zip\n",
      "Reading Compressed CSV: ./data/investing_comments.csv.zip\n",
      "Reading Compressed CSV: ./data/pennystocks_comments.csv.zip\n",
      "Reading Compressed CSV: ./data/stockmarket_comments.csv.zip\n",
      "Reading Compressed CSV: ./data/options_comments.csv.zip\n",
      "Reading Compressed CSV: ./data/robinhoodpennystocks_comments.csv.zip\n",
      "Reading Compressed CSV: ./data/wallstreetbetsnew_comments.csv.zip\n",
      "Reading Compressed CSV: ./data/smallstreetbets_comments.csv.zip\n",
      "Files Loaded: 40.96s\n",
      "Total Records: 4432533\n"
     ]
    }
   ],
   "source": [
    "#//*** Get all Reddit Comments and merge into a single Collection\n",
    "\n",
    "#process_tfidf = False\n",
    "\n",
    "#//**** Aggregate comments into a single dataframe\n",
    "raw_df = build_all_comments(subreddits,filepath,filename_suffix)\n",
    "\n",
    "raw_df\n",
    "\n",
    "#//*** Trim dataframe to start in January 20202-01-02\n",
    "start_trim_date = pd.to_datetime(\"2020-01-02\")\n",
    "raw_df = raw_df[raw_df['created_utc'] >= start_trim_date]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>is_submitter</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11770</th>\n",
       "      <td>2020-01-31 15:56:13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>t2_3yo4vdyj</td>\n",
       "      <td>I thought he didn’t kill people 🤔</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11771</th>\n",
       "      <td>2020-01-31 15:56:17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>t2_127kom</td>\n",
       "      <td>Looks like 4pm is where your chart ends at the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11772</th>\n",
       "      <td>2020-01-31 15:56:19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>t2_49f6b0hp</td>\n",
       "      <td>Notice how I said Trump’s America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11773</th>\n",
       "      <td>2020-01-31 15:56:21</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>t2_56mvp9vo</td>\n",
       "      <td>If trump is exhonerated this weekend 🚀🚀🚀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11774</th>\n",
       "      <td>2020-01-31 15:56:22</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>t2_15yfqh</td>\n",
       "      <td>Lol after yesterday I'm going monthly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4432528</th>\n",
       "      <td>2021-06-30 20:34:33</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>t2_8kmwyads</td>\n",
       "      <td>Jun 30, 9.30pm EST.\\n\\n  \\nI just saw this pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4432529</th>\n",
       "      <td>2021-06-30 21:08:36</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>t2_ah5dhj6a</td>\n",
       "      <td>Good run through. The mill is running and we’r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4432530</th>\n",
       "      <td>2021-06-30 22:11:10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>t2_6ypqa</td>\n",
       "      <td>Back to normal not there yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4432531</th>\n",
       "      <td>2021-06-30 22:16:51</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>t2_9ypzayi0</td>\n",
       "      <td>💎👐🚀🚀🚀🚀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4432532</th>\n",
       "      <td>2021-06-30 22:40:35</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>t2_9t61te74</td>\n",
       "      <td>I'll accept that.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4289460 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                created_utc  score  total_awards_received  is_submitter  \\\n",
       "11770   2020-01-31 15:56:13      1                    0.0          True   \n",
       "11771   2020-01-31 15:56:17      1                    0.0          True   \n",
       "11772   2020-01-31 15:56:19      1                    0.0          True   \n",
       "11773   2020-01-31 15:56:21      1                    0.0          True   \n",
       "11774   2020-01-31 15:56:22      1                    0.0          True   \n",
       "...                     ...    ...                    ...           ...   \n",
       "4432528 2021-06-30 20:34:33      1                    0.0          True   \n",
       "4432529 2021-06-30 21:08:36      1                    0.0         False   \n",
       "4432530 2021-06-30 22:11:10      0                    0.0         False   \n",
       "4432531 2021-06-30 22:16:51      1                    0.0         False   \n",
       "4432532 2021-06-30 22:40:35      2                    0.0         False   \n",
       "\n",
       "        author_fullname                                               body  \n",
       "11770       t2_3yo4vdyj                  I thought he didn’t kill people 🤔  \n",
       "11771         t2_127kom  Looks like 4pm is where your chart ends at the...  \n",
       "11772       t2_49f6b0hp                  Notice how I said Trump’s America  \n",
       "11773       t2_56mvp9vo           If trump is exhonerated this weekend 🚀🚀🚀  \n",
       "11774         t2_15yfqh              Lol after yesterday I'm going monthly  \n",
       "...                 ...                                                ...  \n",
       "4432528     t2_8kmwyads  Jun 30, 9.30pm EST.\\n\\n  \\nI just saw this pos...  \n",
       "4432529     t2_ah5dhj6a  Good run through. The mill is running and we’r...  \n",
       "4432530        t2_6ypqa                       Back to normal not there yet  \n",
       "4432531     t2_9ypzayi0                                             💎👐🚀🚀🚀🚀  \n",
       "4432532     t2_9t61te74                                  I'll accept that.  \n",
       "\n",
       "[4289460 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "==========\n",
      "==========\n",
      "Building clov Targets...\n",
      "Dropping clov dates before 2020-01-02 00:00:00\n",
      "Dropping clov dates after last comment 2021-06-30 22:40:35\n",
      "Merging clov with comments\n",
      "Processing...\n",
      "Merge Built: 6.85s\n",
      "Begin clov tfidf....\n",
      "tfidf Built: 0.02s\n",
      "Pickling clov tfidf...\n",
      "Pickling clov tfidf_matrix\n",
      "Saving clov stocks with target(s)\n",
      "clov Processing Complete: 7.29\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "Building sofi Targets...\n",
      "Dropping sofi dates before 2020-01-02 00:00:00\n",
      "Dropping sofi dates after last comment 2021-06-30 22:40:35\n",
      "Merging sofi with comments\n",
      "Processing...\n",
      "Merge Built: 4.6s\n",
      "Begin sofi tfidf....\n",
      "tfidf Built: 0.02s\n",
      "Pickling sofi tfidf...\n",
      "Pickling sofi tfidf_matrix\n",
      "Saving sofi stocks with target(s)\n",
      "sofi Processing Complete: 4.89\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "Building wkhs Targets...\n",
      "Dropping wkhs dates before 2020-01-02 00:00:00\n",
      "Dropping wkhs dates after last comment 2021-06-30 22:40:35\n",
      "Merging wkhs with comments\n",
      "Processing...\n",
      "Merge Built: 8.55s\n",
      "Begin wkhs tfidf....\n",
      "tfidf Built: 0.01s\n",
      "Pickling wkhs tfidf...\n",
      "Pickling wkhs tfidf_matrix\n",
      "Saving wkhs stocks with target(s)\n",
      "wkhs Processing Complete: 8.87\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "Building amd Targets...\n",
      "Dropping amd dates before 2020-01-02 00:00:00\n",
      "Dropping amd dates after last comment 2021-06-30 22:40:35\n",
      "Merging amd with comments\n",
      "Processing...\n",
      "Merge Built: 9.7s\n",
      "Begin amd tfidf....\n",
      "tfidf Built: 0.08s\n",
      "Pickling amd tfidf...\n",
      "Pickling amd tfidf_matrix\n",
      "Saving amd stocks with target(s)\n",
      "amd Processing Complete: 10.06\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "Building gme Targets...\n",
      "Dropping gme dates before 2020-01-02 00:00:00\n",
      "Dropping gme dates after last comment 2021-06-30 22:40:35\n",
      "Merging gme with comments\n",
      "Processing...\n",
      "Merge Built: 9.45s\n",
      "Begin gme tfidf....\n",
      "tfidf Built: 0.46s\n",
      "Pickling gme tfidf...\n",
      "Pickling gme tfidf_matrix\n",
      "Saving gme stocks with target(s)\n",
      "gme Processing Complete: 10.2\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "Building x Targets...\n",
      "Dropping x dates before 2020-01-02 00:00:00\n",
      "Dropping x dates after last comment 2021-06-30 22:40:35\n",
      "Merging x with comments\n",
      "Processing...\n",
      "Merge Built: 9.38s\n",
      "Begin x tfidf....\n",
      "tfidf Built: 20.11s\n",
      "Pickling x tfidf...\n",
      "Pickling x tfidf_matrix\n",
      "Saving x stocks with target(s)\n",
      "x Processing Complete: 29.86\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "Building amc Targets...\n",
      "Dropping amc dates before 2020-01-02 00:00:00\n",
      "Dropping amc dates after last comment 2021-06-30 22:40:35\n",
      "Merging amc with comments\n",
      "Processing...\n",
      "Merge Built: 9.58s\n",
      "Begin amc tfidf....\n",
      "tfidf Built: 0.19s\n",
      "Pickling amc tfidf...\n",
      "Pickling amc tfidf_matrix\n",
      "Saving amc stocks with target(s)\n",
      "amc Processing Complete: 10.13\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "Building clne Targets...\n",
      "Dropping clne dates before 2020-01-02 00:00:00\n",
      "Dropping clne dates after last comment 2021-06-30 22:40:35\n",
      "Merging clne with comments\n",
      "Processing...\n",
      "Merge Built: 8.1s\n",
      "Begin clne tfidf....\n",
      "tfidf Built: 0.01s\n",
      "Pickling clne tfidf...\n",
      "Pickling clne tfidf_matrix\n",
      "Saving clne stocks with target(s)\n",
      "clne Processing Complete: 8.41\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "Building nio Targets...\n",
      "Dropping nio dates before 2020-01-02 00:00:00\n",
      "Dropping nio dates after last comment 2021-06-30 22:40:35\n",
      "Merging nio with comments\n",
      "Processing...\n",
      "Merge Built: 10.07s\n",
      "Begin nio tfidf....\n",
      "tfidf Built: 1.26s\n",
      "Pickling nio tfidf...\n",
      "Pickling nio tfidf_matrix\n",
      "Saving nio stocks with target(s)\n",
      "nio Processing Complete: 11.65\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "Building mu Targets...\n",
      "Dropping mu dates before 2020-01-02 00:00:00\n",
      "Dropping mu dates after last comment 2021-06-30 22:40:35\n",
      "Merging mu with comments\n",
      "Processing...\n",
      "Merge Built: 9.61s\n",
      "Begin mu tfidf....\n",
      "tfidf Built: 8.42s\n",
      "Pickling mu tfidf...\n",
      "Pickling mu tfidf_matrix\n",
      "Saving mu stocks with target(s)\n",
      "mu Processing Complete: 18.36\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "Building spce Targets...\n",
      "Dropping spce dates before 2020-01-02 00:00:00\n",
      "Dropping spce dates after last comment 2021-06-30 22:40:35\n",
      "Merging spce with comments\n",
      "Processing...\n",
      "Merge Built: 9.26s\n",
      "Begin spce tfidf....\n",
      "tfidf Built: 0.02s\n",
      "Pickling spce tfidf...\n",
      "Pickling spce tfidf_matrix\n",
      "Saving spce stocks with target(s)\n",
      "spce Processing Complete: 9.57\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "Building bb Targets...\n",
      "Dropping bb dates before 2020-01-02 00:00:00\n",
      "Dropping bb dates after last comment 2021-06-30 22:40:35\n",
      "Merging bb with comments\n",
      "Processing...\n",
      "Merge Built: 9.4s\n",
      "Begin bb tfidf....\n",
      "tfidf Built: 1.21s\n",
      "Pickling bb tfidf...\n",
      "Pickling bb tfidf_matrix\n",
      "Saving bb stocks with target(s)\n",
      "bb Processing Complete: 10.91\n",
      "All Processing Complete : 140.2s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "#//**** Get all Stocks to Model\n",
    "interval=\"daily\"\n",
    "\n",
    "#//*** Path to processed files\n",
    "\n",
    "#//*** Path to the stock ticker JSON file\n",
    "stock_ticker_filename = \"./data/stock_tickers.json\"\n",
    "\n",
    "#//*** Convert Path to Mac formatting if needed\n",
    "#if platform.system() == 'Darwin':\n",
    "#    output_filename = output_filename.replace(\"\\\\\",\"/\")\n",
    "#    stock_ticker_filename = stock_ticker_filename.replace(\"\\\\\",\"/\")\n",
    "\n",
    "#//*** Load the Stock Tickers\n",
    "f = open(stock_ticker_filename, \"r\")\n",
    "symbols = json.loads(f.read())['symbols']\n",
    "\n",
    "#symbols = [\"amc\"]\n",
    "f.close()\n",
    "\n",
    "#//*** Convert symbols to lower case\n",
    "symbols = [x.lower() for x in symbols]\n",
    "\n",
    "start_process_time = time.time()\n",
    "\n",
    "for symbol in symbols:\n",
    "    start_symbol_time = time.time()\n",
    "    \n",
    "    print(\"==========\")\n",
    "    print(\"==========\")\n",
    "    print(\"==========\")\n",
    "    print(f\"Building {symbol} Targets...\")\n",
    "    #//*** Build stock price and target columns to predict\n",
    "    stock_df = build_stocks(symbol,'daily',[1])  \n",
    "\n",
    "    print(f\"Dropping {symbol} dates before {start_trim_date}\")\n",
    "    stock_df = stock_df[stock_df['time'] >= start_trim_date]\n",
    "\n",
    "    print(f\"Dropping {symbol} dates after last comment {raw_df['created_utc'].iloc[-1]}\")\n",
    "    #//*** Remove stocks that are older than comments\n",
    "    stock_df = stock_df[stock_df['time'] <= raw_df['created_utc'].iloc[-1]]\n",
    "    \n",
    "    print(f\"Merging {symbol} with comments\")\n",
    "    #//*** Combine\n",
    "    model_df = group_dataframe_by_time(stock_df,raw_df,symbol)\n",
    "    \n",
    "    \n",
    "    model_df['body'] = model_df['body'].replace(np.nan, \" \")\n",
    "    \n",
    "    tfidf = TfidfVectorizer()\n",
    "\n",
    "    print(f\"Begin {symbol} tfidf....\")\n",
    "    start_time = time.time()\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf.fit_transform(model_df['body'])\n",
    "    print(f\"tfidf Built: {round(time.time()-start_time,2)}s\")\n",
    "    \n",
    "    print(f\"Pickling {symbol} tfidf...\")\n",
    "    outfile = open(f\"./ignore_folder/model_ready_{symbol}_{interval}_tfidf_.pkl\",\"wb\")\n",
    "    pickle.dump(tfidf,outfile)\n",
    "    outfile.close()\n",
    "    print(f\"Pickling {symbol} tfidf_matrix\")\n",
    "    outfile = open(f\"./ignore_folder/model_ready_{symbol}_{interval}_tfidf_matrix.pkl\",\"wb\")\n",
    "    pickle.dump(tfidf_matrix,outfile)\n",
    "    outfile.close()\n",
    "    \n",
    "    print(f\"Saving {symbol} stocks with target(s)\")\n",
    "    del model_df['body']\n",
    "    del model_df['author_fullname']\n",
    "    #//*** Drop the comments and comment count and save for modeling\n",
    "    model_df.to_csv(f\"./ignore_folder/model_ready_{symbol}_{interval}.csv.zip\",compression=\"zip\",index=False)\n",
    "    \n",
    "    print(f\"{symbol} Processing Complete: {round(time.time()-start_symbol_time,2)}\")\n",
    "    \n",
    "\n",
    "print (f\"All Processing Complete : {round(time.time()-start_process_time,2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'body'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'body'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e8c5f684e271>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'body'"
     ]
    }
   ],
   "source": [
    "model_df['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gg = pd.read_csv(f\"./data/model_ready_{symbol}_{interval}.csv.zip\")\n",
    "gg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Group the stock values by time, this is essentially like itterrows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.plot(model_df['time'],model_df['comment_count'] )\n",
    "\n",
    "\n",
    "\n",
    "    #plt.legend(loc='upper right',bbox_to_anchor=(1.35, 1.2))\n",
    "plt.show()\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.plot(model_df['time'],model_df['comment_count'] )\n",
    "\n",
    "\n",
    "\n",
    "    #plt.legend(loc='upper right',bbox_to_anchor=(1.35, 1.2))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import time\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=['target','rmse','r2','start','mid','end','actual','predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = pd.DataFrame(columns=['time','actual','predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#//*** Lengthy K-Means Run. Runs a model on every day after 60+ and predicts the next day. Uses the previous 60 days to predict the next day. \n",
    "#//*** It may or may not be helpful to run k-means across the whole tfidf instead of the previous 60.\n",
    "\n",
    "predict_df = pd.DataFrame(columns=['time','actual','predict','best_r'])\n",
    "\n",
    "#//*** Loop through model_df. Model each day individually. Train 60 days, Predict 61.\n",
    "training_days = 60\n",
    "train_start_slice = 0\n",
    "train_end_slice = training_days\n",
    "cluster_iterations = 50\n",
    "predict_col = [f'target_1']\n",
    "train_cols = ['cluster_1','close','volume','open','high','low']\n",
    "#Loop through combined \n",
    "for row_int in range(0,len(tdf)-training_days-1):\n",
    "    \n",
    "    \n",
    "    #//*** Build the slice start index\n",
    "    start_dex = train_start_slice+row_int\n",
    "    \n",
    "    #//*** Build the slice end index\n",
    "    end_dex = train_end_slice+row_int\n",
    "    \n",
    "    dc = f\"{start_dex}/{len(model_df)}\"\n",
    "\n",
    "    #//*** Build the sliced training df\n",
    "    train_df = model_df[start_dex:end_dex].copy()\n",
    "\n",
    "    tfidf = TfidfVectorizer()\n",
    "\n",
    "    #print(dc, \" Starting tfidf....\")\n",
    "    start_time = time.time()\n",
    "    tfidf = TfidfVectorizer()\n",
    "    train_tfidf = tfidf.fit_transform(train_df['body'])\n",
    "    print(dc,f\"tfidf Built: {round(time.time()-start_time,2)}s\")\n",
    "    \n",
    "    \n",
    "    #//*** Build the row index on the prediction (train_end_slice+1)\n",
    "    predict_dex = row_int+train_end_slice+1\n",
    "    \n",
    "    #print(\"Starting K-Means...\")\n",
    "    offset=1\n",
    "    best_r = 0\n",
    "    best_cluster = []\n",
    "    for x in range(1,cluster_iterations):\n",
    "\n",
    "        start_time=time.time()\n",
    "        kmeans = KMeans(n_clusters=5,init='random').fit(train_tfidf)\n",
    "        cluster = kmeans.predict(train_tfidf)\n",
    "        r,p = pearsonr(train_df[f'target_{offset}'],cluster)\n",
    "        r = abs(r)\n",
    "        #print(r,round(p,6))\n",
    "\n",
    "        if p < .05:\n",
    "            if r > best_r:\n",
    "                best_r = r\n",
    "                best_cluster = cluster\n",
    "            #plt.legend(loc='upper right',bbox_to_anchor=(1.35, 1.2))\n",
    "        if x == 1:\n",
    "            print(dc,f\"Estimated Cluster Run Time: {round( (time.time()-start_time)*cluster_iterations-1,2) }s\")\n",
    "    #print(f\"Best R:{best_r}\")\n",
    "     \n",
    "    train_df[f'cluster_{offset}'] = best_cluster\n",
    "\n",
    "    \n",
    "    x_train = np.array(train_df[train_cols])\n",
    "    y_train = np.array(train_df[predict_col])\n",
    "\n",
    "    x_test = pd.DataFrame(model_df.iloc[end_dex+1].copy()).transpose()\n",
    "    \n",
    "    x_test[f'cluster_{offset}'] = best_cluster[0]\n",
    "    x_test = np.array(x_test[train_cols])\n",
    "    \n",
    "\n",
    "    y_test = model_df[predict_col].iloc[end_dex:end_dex+1]\n",
    "\n",
    "    #regr_iter = 20\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create linear regression object\n",
    "    regr = linear_model.LinearRegression()\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(x_train, y_train)\n",
    "    result = regr.predict(x_test)\n",
    "\n",
    "\n",
    "\n",
    "    print(dc,f\"[{result[0][0]}]  Actual: {model_df.iloc[predict_dex]['close']} - Best R2: {best_r}\")\n",
    "    \n",
    "    \n",
    "    #//*** Add Results to the predict_df\n",
    "    predict_df.loc[len(predict_df)] = [model_df.iloc[predict_dex]['time'],model_df.iloc[predict_dex]['close'],result[0][0],best_r]\n",
    "\n",
    "print(\"Done!\")\n",
    "predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df.to_csv(\"./results/amc_daily_60_to_1_model.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(20)\n",
    "plot_x = np.arange(len(y_test))\n",
    "ax.plot(predict_df['time'],predict_df['actual'] )\n",
    "ax.scatter(predict_df['time'],predict_df['predict'],color='red' )\n",
    "plt.suptitle(f\"AMC Model Train 60: Predict 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = (model_df.iloc[end_dex+1].copy())\n",
    "x_test = pd.DataFrame(model_df.iloc[end_dex+1].copy()).transpose()\n",
    "x_test[f'cluster_{offset}'] = best_cluster[0]\n",
    "#x_test = x_test.append( pd.Series(best_cluster[0], index = [f'cluster_{offset}']) )\n",
    "x_test\n",
    "x_train\n",
    "x_train = np.array(train_df[train_cols].iloc[start_dex:end_dex])\n",
    "y_train = np.array(train_df.iloc[start_dex:end_dex][predict_col],)\n",
    "train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#//*** Train 60\n",
    "#//*** Model 30\n",
    "#//*** The Original K-Means Model\n",
    "\n",
    "\n",
    "#//*** Divide up the data in 60 day intervals to predict the next 30 days\n",
    "days = 30 \n",
    "train_cols = ['cluster_1','close','volume','open','high','low']\n",
    "cluster_iterations = 50\n",
    "\n",
    "\n",
    "for index in range(2,int(len(tdf)/days)):\n",
    "    \n",
    "    min_slice = ((index-2)*days)\n",
    "    mid_slice = index*days\n",
    "    max_slice = (index*days)+days\n",
    "    print(index,\"Train Range:\",min_slice,mid_slice,\" - Test Range:\", mid_slice, max_slice  )\n",
    "    \n",
    "    train_df = tdf[min_slice:max_slice].copy()\n",
    "\n",
    "    tfidf = TfidfVectorizer()\n",
    "\n",
    "    print(\"Starting tfidf....\")\n",
    "    start_time = time.time()\n",
    "    tfidf = TfidfVectorizer()\n",
    "    train_tfidf = tfidf.fit_transform(train_df['body'])\n",
    "    print(f\"Built: {round(time.time()-start_time,2)}\")\n",
    "    \n",
    "    print(\"Starting K-Means...\")\n",
    "    offset=1\n",
    "    best_r = 0\n",
    "    best_cluster = []\n",
    "    for x in range(1,cluster_iterations):\n",
    "\n",
    "        start_time=time.time()\n",
    "        kmeans = KMeans(n_clusters=5,init='random').fit(train_tfidf)\n",
    "        cluster = kmeans.predict(train_tfidf)\n",
    "        r,p = pearsonr(train_df[f'target_{offset}'],cluster)\n",
    "        r = abs(r)\n",
    "        #print(r,round(p,6))\n",
    "\n",
    "        if p < .05:\n",
    "            if r > best_r:\n",
    "                best_r = r\n",
    "                best_cluster = cluster\n",
    "            #plt.legend(loc='upper right',bbox_to_anchor=(1.35, 1.2))\n",
    "        if x == 1:\n",
    "            print(f\"Estimated Cluster Run Time: {round( (time.time()-start_time)*cluster_iterations-1,2) }s\")\n",
    "    print(f\"Best R:{best_r}\")\n",
    "    train_df[f'cluster_{offset}'] = best_cluster\n",
    "    \n",
    "     \n",
    "    for tgt in [1,2,3,4,5,6,7,8,9,10]:\n",
    "        predict_col = [f'target_{tgt}']\n",
    "        x_train = np.array(train_df[train_cols].iloc[0:days*2*-1])\n",
    "        y_train = np.array(train_df.iloc[0:days*2*-1][predict_col],)\n",
    "\n",
    "        x_test = train_df[train_cols].iloc[days*2*-1:]\n",
    "        y_test = train_df[predict_col].iloc[days*2*-1:]\n",
    "\n",
    "        regr_iter = 20\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Create linear regression object\n",
    "        regr = linear_model.LinearRegression()\n",
    "\n",
    "        # Train the model using the training sets\n",
    "        regr.fit(x_train, y_train)\n",
    "        result = regr.predict(x_test)\n",
    "\n",
    "        mse = mean_squared_error(y_test, result)\n",
    "        r2 = r2_score(y_test,result)\n",
    "        \n",
    "        #//*** Root Mean squared Error\n",
    "        rmse = sqrt(mse)\n",
    "\n",
    "        # Plot outputs\n",
    "        display_size = 40\n",
    "\n",
    "        fig,ax = plt.subplots()\n",
    "        plot_x = np.arange(len(y_test))\n",
    "        ax.plot(plot_x,y_test )\n",
    "        ax.scatter(plot_x,result,color='red' )\n",
    "        plt.suptitle(f\"{predict_col}\\nRmse: {rmse} - r2 {r2}\")\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "\n",
    "        #print(rmse)\n",
    "        #print(result)\n",
    "\n",
    "        #//*** Update Results_df\n",
    "        results_df.loc[len(results_df)] = [predict_col[0],rmse,r2,min_slice,mid_slice,max_slice,np.array(y_test),result]\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_df.to_pickle(\"./results_amc_daily_kmeans_ols.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_df)\n",
    "tfidf = TfidfVectorizer()\n",
    "whole_tfidf = tfidf.fit_transform(model_df['body'])\n",
    "whole_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = whole_tfidf.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_size = 40\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(20)\n",
    "plot_x = np.arange(len(t[0]))\n",
    "for x in t:\n",
    "    ax.scatter(plot_x,x,s=1 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_df = pd.DataFrame()\n",
    "vocab_df['word'] = tfidf.vocabulary_\n",
    "vocab_df['score'] = tfidf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(whole_tfidf.toarray())\n",
    "tfidf_df.columns = tfidf.vocabulary_\n",
    "\n",
    "#tfidf_df[tfidf_df.columns[:10]]\n",
    "\n",
    "\n",
    "list(vocab_df.sort_values('score',ascending=True)['word'])\n",
    "#tfidf_df.columns = (range(len(tfidf_df.columns)))\n",
    "tfidf_df[tfidf_df.columns[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_df = pd.DataFrame()\n",
    "vocab_df['word'] = tfidf.vocabulary_\n",
    "vocab_df['score'] = tfidf.idf_\n",
    "tfidf_df = pd.DataFrame(whole_tfidf.toarray())\n",
    "ascending_vals = list(tfidf_df.sum().sort_values(ascending=True).index)\n",
    "descending_vals = list(tfidf_df.sum().sort_values(ascending=False).index)\n",
    "vocab_vals = list(vocab_df.sort_values('score',ascending=True)['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "tsvd = TruncatedSVD(3)\n",
    "tsvd_df = pd.DataFrame(tsvd.fit_transform(tfidf_df[descending_vals[:100]]))\n",
    "print(tsvd.explained_variance_ratio_.sum())\n",
    "tsvd_df**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tfidf_df[descending_vals[:1000]]\n",
    "\n",
    "train_stock_cols = ['close','volume','open','high','low']\n",
    "train_stock_cols = ['close']\n",
    "model_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "#//*** Combine the model columns for training with the tfidf columns\n",
    "X = np.array(pd.concat([model_df[train_stock_cols],tfidf_df[descending_vals[:100]]],ignore_index=True,axis=1))\n",
    "X = np.array(pd.concat([model_df[train_stock_cols],tsvd_df],ignore_index=True,axis=1))\n",
    "X = np.array(pd.concat([tsvd_df**2,model_df[train_stock_cols]],ignore_index=True,axis=1))\n",
    "#X = model_df[train_stock_cols]\n",
    "y = model_df['target_1']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2, shuffle=False)\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(x_train, y_train)\n",
    "result = regr.predict(x_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, result)\n",
    "r2 = r2_score(y_test,result)\n",
    "\n",
    "#//*** Root Mean squared Error\n",
    "rmse = sqrt(mse)\n",
    "\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(20)\n",
    "plot_x = np.arange(len(y_test))\n",
    "ax.plot(plot_x,y_test )\n",
    "ax.scatter(plot_x,result,color='red' )\n",
    "#ax.scatter(predict_df['time'],predict_df['predict'],color='red' )\n",
    "plt.suptitle(f\"r2: {r2} rmse: {rmse}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_test = pd.DataFrame(model_df.iloc[end_dex+1].copy()).transpose()\n",
    "\n",
    "x_test[f'cluster_{offset}'] = best_cluster[0]\n",
    "x_test = np.array(x_test[train_cols])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_test = model_df[predict_col].iloc[end_dex:end_dex+1]\n",
    "\n",
    "#regr_iter = 20\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = tfidf_df[descending_vals]\n",
    "display_size = 40\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(20)\n",
    "plot_x = np.arange(len(plot_df.columns))\n",
    "for x in range(len(plot_df)):\n",
    "    ax.scatter(plot_x,plot_df.iloc[x],s=1 )\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(20)\n",
    "plot_x = np.arange(len(plot_df.columns[:1000]))\n",
    "for x in range(len(plot_df)):\n",
    "    ax.scatter(plot_x,plot_df[plot_df.columns[:1000]].iloc[x],s=1 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in plot_df[plot_df.columns[:100]]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_size = 40\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(20)\n",
    "plot_x = np.arange(len(plot_df.columns))\n",
    "ax.scatter(plot_df.index,plot_df[plot_df.columns[0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = tfidf_df[vocab_vals]\n",
    "display_size = 40\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(20)\n",
    "plot_x = np.arange(len(plot_df.columns))\n",
    "for x in range(len(plot_df)):\n",
    "    ax.scatter(plot_x,plot_df.iloc[x],s=1 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_size = 40\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(20)\n",
    "plot_x = np.arange(len(tfidf_df.columns))\n",
    "\n",
    "#for x in range(len(tfidf_df)):\n",
    "ax.scatter(plot_x,tfidf_df.iloc[0][descending_vals],s=5,color='green' )\n",
    "#tfidf_df.columns = tfidf_df.sum().sort_values(ascending=True).index\n",
    "ax.scatter(plot_x,tfidf_df.iloc[0][ascending_vals],s=2,color='black' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_size = 40\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "fig.set_figheight(100)\n",
    "fig.set_figwidth(200)\n",
    "plot_x = np.arange(len(tfidf_df.columns))\n",
    "for x in range(len(tfidf_df)):\n",
    "    ax.scatter(plot_x,tfidf_df.iloc[x],s=1 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "print(\"Finding best cluster for classification\")\n",
    "\n",
    "train_slice = (270,300)\n",
    "train_interval = 10\n",
    "\n",
    "cluster_iterations = 50\n",
    "\n",
    "train_df = tdf[train_slice[0]:train_slice[1]+train_interval].copy()\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "print(\"Starting tfidf....\")\n",
    "start_time = time.time()\n",
    "tfidf = TfidfVectorizer()\n",
    "train_tfidf = tfidf.fit_transform(train_df['body'])\n",
    "\n",
    "\n",
    "print(f\"Built: {round(time.time()-start_time,2)}\")\n",
    "print(\"Starting K-Means...\")\n",
    "offset=1\n",
    "best_r = 0\n",
    "best_cluster = []\n",
    "for x in range(1,cluster_iterations):\n",
    "    \n",
    "    start_time=time.time()\n",
    "    kmeans = KMeans(n_clusters=5,init='random').fit(train_tfidf)\n",
    "    cluster = kmeans.predict(train_tfidf)\n",
    "    r,p = pearsonr(train_df[f'target_{offset}'],cluster)\n",
    "    r = abs(r)\n",
    "    #print(r,round(p,6))\n",
    "    \n",
    "    if p < .05:\n",
    "        if r > best_r:\n",
    "            best_r = r\n",
    "            best_cluster = cluster\n",
    "        #plt.legend(loc='upper right',bbox_to_anchor=(1.35, 1.2))\n",
    "    if x == 1:\n",
    "        print(f\"Estimated Cluster Run Time: {round( (time.time()-start_time)*cluster_iterations-1,2) }s\")\n",
    "print(best_r,best_cluster)\n",
    "train_df[f'cluster_{offset}'] = best_cluster\n",
    "train_df\n",
    "   \n",
    "   \"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\"\"\"\n",
    "\n",
    "train_cols = ['cluster_1','close','volume','open','high','low']\n",
    "predict_col = ['target_1']\n",
    "x_train = np.array(train_df[train_cols].iloc[0:train_interval*-1])\n",
    "y_train = np.array(train_df.iloc[0:train_interval*-1][predict_col],)\n",
    "\n",
    "x_test = train_df[train_cols].iloc[train_interval*-1:]\n",
    "y_test = train_df[predict_col].iloc[train_interval*-1:]\n",
    "\n",
    "\n",
    "import time\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "regr_iter = 20\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Regressing\")\n",
    "\n",
    "\n",
    "#regr = MLPRegressor(max_iter=50000).fit(x_train,y_train)\n",
    "#scores = cross_val_score(regr, x_train, y_train, cv=5)\n",
    "print(f\"Complete: {round(time.time()-start_time,2)}\" )\n",
    "#//*** Score the model\n",
    "#score = regr.score(x_train, y_train)\n",
    "#result = regr.predict(x_test)\n",
    "\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(x_train, y_train)\n",
    "result = regr.predict(x_test)\n",
    "# Plot outputs\n",
    "display_size = 40\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "plot_x = np.arange(len(y_test))\n",
    "ax.plot(plot_x,y_test )\n",
    "ax.scatter(plot_x,result,color='red' )\n",
    "\n",
    "plt.show()\n",
    "mse = mean_squared_error(y_test, result)\n",
    "\n",
    "#//*** Root Mean squared Error\n",
    "rmse = sqrt(mse)\n",
    "\n",
    "print(rmse)\n",
    "print(result)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//***Aggreate Comments for Training\n",
    "#//*** Build tfidf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "#loop_list.append(tfidf.fit_transform(input_df['tfidf']))\n",
    "tfidf_matrix = []\n",
    "tfidf_list = []\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "print(\"Starting tfidf....\")\n",
    "start_time = time.time()\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(tdf['body'])\n",
    "\n",
    "\n",
    "print(f\"Built: {round(time.time()-start_time,2)}\")\n",
    "\n",
    "print(tfidf_matrix)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#//*** Build TruncatedSVD\n",
    "\n",
    "#from sklearn.decomposition import PCA\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "start_time = time.time()\n",
    "print(f\"Begin Truncated SVD \")\n",
    "\n",
    "start_time=time.time()\n",
    "#//*** Set the number of components to 6000. This is generating 98% variance capture\n",
    "#//*** 60min data set took around 25minutes to build\n",
    "tsvd = TruncatedSVD(250)\n",
    "tsvd_df = pd.DataFrame(tsvd.fit_transform(tfidf_matrix))\n",
    "print(tsvd.explained_variance_ratio_.sum())\n",
    "\n",
    "print (f\"Truncated SVD Done: {round(time.time()-start_time,2)}s\")\n",
    "#output_filename = './ignore_folder/tsvd_model_ready_daily.csv.zip'\n",
    "#//*** Write Truncated SVD to disk\n",
    "#tsvd_df.to_csv(output_filename, compression='zip', index=False)\n",
    "\"\"\"\n",
    "print(\"Truncated SVD Code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LTSM: https://www.datacamp.com/community/tutorials/lstm-python-stock-market\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "\n",
    "for col in tsvd_df.columns:\n",
    "\n",
    "    ax.scatter(tsvd_df[col],tsvd_df.index,label=col )\n",
    "\n",
    "\n",
    "    #plt.legend(loc='upper right',bbox_to_anchor=(1.35, 1.2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# function returns WSS score for k values from 1 to kmax\n",
    "def calculate_WSS(points, kmax):\n",
    "  sse = []\n",
    "  for k in range(1, kmax+1):\n",
    "    kmeans = KMeans(n_clusters = k).fit(points)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    pred_clusters = kmeans.predict(points)\n",
    "    curr_sse = 0\n",
    "    \n",
    "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
    "    for i in range(len(points)):\n",
    "        curr_center = centroids[pred_clusters[i]]\n",
    "        curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
    "      \n",
    "    sse.append(curr_sse)\n",
    "    return sse\n",
    "\n",
    "dir(tfidf_matrix)\n",
    "#kmeans = KMeans(n_clusters=2).fit(tfidf_matrix)\n",
    "dir(kmeans)\n",
    "kmeans.score(tfidf_matrix)\n",
    "k_scores = []\n",
    "for x in [5,10,15,20,25,50]:\n",
    "    start_time=time.time()\n",
    "    kmeans = KMeans(n_clusters=x).fit(tfidf_matrix)\n",
    "    loop_score = kmeans.score(tfidf_matrix)\n",
    "    print(f\"{x} - {loop_score} - {time.time()-start_time}s\")\n",
    "    k_scores.append(loop_score)\n",
    "print(k_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.plot(range(len(k_scores)),k_scores )\n",
    "\n",
    "\n",
    "    #plt.legend(loc='upper right',bbox_to_anchor=(1.35, 1.2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kmeans + Mean Shift: https://jamesxli.blogspot.com/2012/03/on-mean-shift-and-k-means-clustering.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
