{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# //*** Imports and Load Data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#//*** Use the whole window in the IPYNB editor\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#//*** Maximize columns and rows displayed by pandas\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//***************************************\n",
    "#//*** Apply Common Cleanup operations\n",
    "#//***************************************\n",
    "#//*** In anticpation that I'll be re-using text cleanup code. I'm adding some robustness to the function.\n",
    "#//*** Adding kwargs to disable features that default to true.\n",
    "#//*** Whether an action is skipped or executed is based on a boolean value stored in action_dict.\n",
    "#//*** Key values will default to true. If code needs to be defaulted to False, a default_false list can be added later\n",
    "#//*** All Boolean kwarg keya are stored in kwarg list. This speeds up the coding of the action_dict.\n",
    "#//*** As Kwargs are added \n",
    "def mr_clean_text(input_series, input_options={}):\n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "    \n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Add some data validation. I'm preparing this function for additional use. I'm checking if future users (ie future me)\n",
    "    #//*** may throw some garbage at this function. Experience has taught me to fail safely wherever possible.\n",
    "\n",
    "    #//*** All kwargs are listed here. These initialize TRUE by default.\n",
    "    key_list = [ \"lower\", \"newline\", \"html\", \"remove_empty\", \"punctuation\" ]\n",
    "    \n",
    "    #//*** Build Action Dictionary\n",
    "    action_dict = { } \n",
    "    \n",
    "    #//*** Build the keys from kwarg_list and default them to TRUE\n",
    "    for key in key_list:\n",
    "        action_dict[key] = True\n",
    "        \n",
    "    #//*** Loop through the input kwargs (if any). Assign the action_dict values based on the kwargs:\n",
    "    for key,value in input_options.items():\n",
    "        print(key,value)\n",
    "        action_dict[key] = value\n",
    "    \n",
    "    \n",
    "    #//*************************************************************************\n",
    "    #//*** The Cleanup/Processing code is a straight lift from DSC550 - Week02\n",
    "    #//*************************************************************************\n",
    "    #//*** Convert to Lower Case, Default to True\n",
    "    if action_dict[\"lower\"]:\n",
    "        input_series = input_series.str.lower()\n",
    "    \n",
    "   \n",
    "    #//*** Remove New Lines\n",
    "    if action_dict[\"newline\"]:\n",
    "        #//*** Rmove \\r\\n\n",
    "        input_series = input_series.str.replace(r'\\r?\\n',\"\")\n",
    "\n",
    "        #//*** Remove \\n new lines\n",
    "        input_series = input_series.str.replace(r'\\n',\"\")\n",
    "\n",
    "    #//*** Remove html entities, observed entities are &gt; and &lt;. All HTML entities begin with & and end with ;.\n",
    "    #//*** Let's use regex to remove html entities\n",
    "    if action_dict[\"html\"]:\n",
    "        input_series = input_series.str.replace(r'&.*;',\"\")\n",
    "\n",
    "    #//*** Remove the empty lines\n",
    "    if action_dict[\"remove_empty\"]:\n",
    "        input_series = input_series[ input_series.str.len() > 0]\n",
    "\n",
    "    #//*** Remove punctuation\n",
    "    if action_dict[\"punctuation\"]:\n",
    "        #//*** Load libraries for punctuation if not already loaded.\n",
    "        #//*** Wrapping these in a try, no sense in importing libraries that already exist.\n",
    "        #//*** Unsure of the cost of reimporting libraries (if any). But testing if library is already loaded feels\n",
    "        #//*** like a good practice\n",
    "        try:\n",
    "            type(sys)\n",
    "        except:\n",
    "            import sys\n",
    "\n",
    "        try:\n",
    "            type(unicodedata)\n",
    "        except:\n",
    "            import unicodedata\n",
    "        \n",
    "        #//*** replace Comma and Period with a space.\n",
    "        for punct in [\",\",\".\",\"$\"]:\n",
    "            input_series = input_series.str.replace(punct,\" \")\n",
    "\n",
    "        #//*** Remove punctuation using the example from the book\n",
    "        punctuation = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P') )\n",
    "        input_series = input_series.str.translate(punctuation)\n",
    "\n",
    "    print(f\"Text Cleaning Time: {time.time() - start_time}\")\n",
    "\n",
    "    return input_series\n",
    "#//*** Remove Stop words from the input list\n",
    "def remove_stop_words(input_series):\n",
    "    \n",
    "    #//*** This function removes stop_words from a series.\n",
    "    #//*** Works with series.apply()\n",
    "    def apply_stop_words(input_list):\n",
    "\n",
    "        #//*** Load Stopwords   \n",
    "        for word in input_list:\n",
    "            if word in stop_words:\n",
    "                input_list.remove(word)\n",
    "        return input_list\n",
    "\n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "        \n",
    "    stopwords = nltk.corpus.stopwords\n",
    "\n",
    "    #//*** Stopwords requires an additional download\n",
    "    try:\n",
    "        type(stopwords)\n",
    "    except:\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "\n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    #//*** The stop_words include punctuation. Stop Word Contractions will not be filtered out.\n",
    "    stop_words = []\n",
    "\n",
    "    #//*** Remove apostrophies from the stop_words\n",
    "    for stop in stopwords.words('english'):\n",
    "        stop_words.append(stop.replace(\"'\",\"\"))\n",
    "\n",
    "    \n",
    "    #//*** Remove Stop words from the tokenized strings in the 'process' column\n",
    "    #input_series = input_series.apply(remove_stop_words,stop_words)\n",
    "    \n",
    "    input_series = input_series.apply(apply_stop_words)\n",
    "\n",
    "    print(f\"Stop Words Time: {time.time() - start_time}\")\n",
    "    \n",
    "    return input_series\n",
    "#//*** Tokenize a Series containing Strings.\n",
    "#//*** Breaking this out into it's own function for later reuse.\n",
    "#//*** Not a lot of code here, but it helps to keep the libraries localized. This creates standarization for future\n",
    "#//*** Stoneburner projects. Also has the ability to add functionality as needed.\n",
    "\n",
    "def tokenize_series(input_series):\n",
    "    \n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "    \n",
    "    word_tokenize = nltk.tokenize.word_tokenize \n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "    \n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    input_series = input_series.apply(word_tokenize)\n",
    "    \n",
    "    print(f\"Tokenize Time: {time.time() - start_time}\")\n",
    "    \n",
    "    return input_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Compressed CSV\n",
      "File Loaded: 2.97s\n",
      "remove_empty False\n",
      "Text Cleaning Time: 4.415989637374878\n",
      "Tokenize Time: 48.00304889678955\n",
      "Stop Words Time: 7.168112754821777\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>is_submitter</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>body</th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>stickied</th>\n",
       "      <th>permalink</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>hash</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-08-24</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>I will accept payments for my research...what'...</td>\n",
       "      <td>c5y9v5z</td>\n",
       "      <td>t3_yqtpn</td>\n",
       "      <td>t1_c5y49wz</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1.429727e+09</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>77c0f32dcf506571815f3d4839454f2b3e550f0e1efecd...</td>\n",
       "      <td>[will, accept, payments, my, research, whats, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-08-24</td>\n",
       "      <td>False</td>\n",
       "      <td>t2_4p1mf</td>\n",
       "      <td>Because previously (until this post), when som...</td>\n",
       "      <td>c5yaaki</td>\n",
       "      <td>t3_yqtpn</td>\n",
       "      <td>t1_c5y9v5z</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1.429728e+09</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>b61949552c3a5d559111ba44d17a71113e408126dd198d...</td>\n",
       "      <td>[previously, post, someone, claimed, looking, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-08-24</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>So you thought I was just going to give all of...</td>\n",
       "      <td>c5yahuo</td>\n",
       "      <td>t3_yqtpn</td>\n",
       "      <td>t1_c5yaaki</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1.429728e+09</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>97bf3c55f77337de3d6b83b05fc5601e2b346845b7b778...</td>\n",
       "      <td>[thought, was, going, give, research, time, aw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-08-24</td>\n",
       "      <td>False</td>\n",
       "      <td>t2_3o5bc</td>\n",
       "      <td>I would also see some proof to back up your cl...</td>\n",
       "      <td>c5yaloj</td>\n",
       "      <td>t3_yqtpn</td>\n",
       "      <td>t1_c5y7jem</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1.429728e+09</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>5a3885658ca462a1fc0dd3b0af8858e183b05f4f474b84...</td>\n",
       "      <td>[would, also, see, proof, back, claims, your, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-08-24</td>\n",
       "      <td>False</td>\n",
       "      <td>t2_4p1mf</td>\n",
       "      <td>&amp;gt; So you thought I was just going to give a...</td>\n",
       "      <td>c5yamfs</td>\n",
       "      <td>t3_yqtpn</td>\n",
       "      <td>t1_c5yahuo</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1.429728e+09</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>405314aad1e814100e822f7ac89274b17d66c4e11e5521...</td>\n",
       "      <td>[because, post, chart, their, site, do, includ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399863</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-06-25</td>\n",
       "      <td>False</td>\n",
       "      <td>t2_9z0opsh3</td>\n",
       "      <td>Nice</td>\n",
       "      <td>h32dw7m</td>\n",
       "      <td>t3_o7jx7p</td>\n",
       "      <td>t1_h329ycg</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/o7jx7p/fraternal_as...</td>\n",
       "      <td>1.624958e+09</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>fdc96ffbf256523aec8846ae56321053c7ab751c99eb76...</td>\n",
       "      <td>[nice]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399864</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-06-25</td>\n",
       "      <td>False</td>\n",
       "      <td>t2_qc6iq</td>\n",
       "      <td>Ah so horoscopes ARE real</td>\n",
       "      <td>h32dwab</td>\n",
       "      <td>t3_o7z71d</td>\n",
       "      <td>t3_o7z71d</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/o7z71d/amc_what_hap...</td>\n",
       "      <td>1.624958e+09</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>20c5bee31dc23c8937aea64670a4157c6547621f763ff4...</td>\n",
       "      <td>[ah, horoscopes, real]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399865</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-06-25</td>\n",
       "      <td>False</td>\n",
       "      <td>t2_bl7b6dkh</td>\n",
       "      <td>If one did this I would tell them Fuck Outta Here</td>\n",
       "      <td>h32dwip</td>\n",
       "      <td>t3_o7vagy</td>\n",
       "      <td>t1_h32daow</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/o7vagy/weekend_disc...</td>\n",
       "      <td>1.624958e+09</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>96ac9390a4f9c8c36aeabf6d3f0358478ce7b3a295c9b6...</td>\n",
       "      <td>[one, this, would, tell, fuck, outta]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399866</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-06-25</td>\n",
       "      <td>False</td>\n",
       "      <td>t2_5b0a37kn</td>\n",
       "      <td>Canada has still those stupid Covid restrictio...</td>\n",
       "      <td>h32dwhz</td>\n",
       "      <td>t3_o7vagy</td>\n",
       "      <td>t3_o7vagy</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/o7vagy/weekend_disc...</td>\n",
       "      <td>1.624958e+09</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>6fa28767c5b75399225dd168a981c8570422fab812afab...</td>\n",
       "      <td>[canada, still, stupid, covid, restrictions, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399867</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-06-25</td>\n",
       "      <td>False</td>\n",
       "      <td>t2_f7cphdq</td>\n",
       "      <td>Go fuck yourself with a cactus. It’s the weeke...</td>\n",
       "      <td>h32dwt5</td>\n",
       "      <td>t3_o7vagy</td>\n",
       "      <td>t1_h32dsig</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/o7vagy/weekend_disc...</td>\n",
       "      <td>1.624958e+09</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>5a9d2e286476b7403cb72369dd669ab680d5380add5ffd...</td>\n",
       "      <td>[go, fuck, with, cactus, the, weekend, fuck, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>399868 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        score  total_awards_received created_utc  is_submitter  \\\n",
       "0           1                    0.0  2012-08-24         False   \n",
       "1           3                    0.0  2012-08-24         False   \n",
       "2          -2                    0.0  2012-08-24         False   \n",
       "3           2                    0.0  2012-08-24         False   \n",
       "4           2                    0.0  2012-08-24         False   \n",
       "...       ...                    ...         ...           ...   \n",
       "399863      3                    0.0  2021-06-25         False   \n",
       "399864      3                    0.0  2021-06-25         False   \n",
       "399865      3                    0.0  2021-06-25         False   \n",
       "399866      4                    0.0  2021-06-25         False   \n",
       "399867      6                    0.0  2021-06-25         False   \n",
       "\n",
       "       author_fullname                                               body  \\\n",
       "0                    0  I will accept payments for my research...what'...   \n",
       "1             t2_4p1mf  Because previously (until this post), when som...   \n",
       "2                    0  So you thought I was just going to give all of...   \n",
       "3             t2_3o5bc  I would also see some proof to back up your cl...   \n",
       "4             t2_4p1mf  &gt; So you thought I was just going to give a...   \n",
       "...                ...                                                ...   \n",
       "399863     t2_9z0opsh3                                               Nice   \n",
       "399864        t2_qc6iq                          Ah so horoscopes ARE real   \n",
       "399865     t2_bl7b6dkh  If one did this I would tell them Fuck Outta Here   \n",
       "399866     t2_5b0a37kn  Canada has still those stupid Covid restrictio...   \n",
       "399867      t2_f7cphdq  Go fuck yourself with a cactus. It’s the weeke...   \n",
       "\n",
       "             id    link_id   parent_id  stickied  \\\n",
       "0       c5y9v5z   t3_yqtpn  t1_c5y49wz     False   \n",
       "1       c5yaaki   t3_yqtpn  t1_c5y9v5z     False   \n",
       "2       c5yahuo   t3_yqtpn  t1_c5yaaki     False   \n",
       "3       c5yaloj   t3_yqtpn  t1_c5y7jem     False   \n",
       "4       c5yamfs   t3_yqtpn  t1_c5yahuo     False   \n",
       "...         ...        ...         ...       ...   \n",
       "399863  h32dw7m  t3_o7jx7p  t1_h329ycg     False   \n",
       "399864  h32dwab  t3_o7z71d   t3_o7z71d     False   \n",
       "399865  h32dwip  t3_o7vagy  t1_h32daow     False   \n",
       "399866  h32dwhz  t3_o7vagy   t3_o7vagy     False   \n",
       "399867  h32dwt5  t3_o7vagy  t1_h32dsig     False   \n",
       "\n",
       "                                                permalink  retrieved_on  \\\n",
       "0                                                       0  1.429727e+09   \n",
       "1                                                       0  1.429728e+09   \n",
       "2                                                       0  1.429728e+09   \n",
       "3                                                       0  1.429728e+09   \n",
       "4                                                       0  1.429728e+09   \n",
       "...                                                   ...           ...   \n",
       "399863  /r/wallstreetbets/comments/o7jx7p/fraternal_as...  1.624958e+09   \n",
       "399864  /r/wallstreetbets/comments/o7z71d/amc_what_hap...  1.624958e+09   \n",
       "399865  /r/wallstreetbets/comments/o7vagy/weekend_disc...  1.624958e+09   \n",
       "399866  /r/wallstreetbets/comments/o7vagy/weekend_disc...  1.624958e+09   \n",
       "399867  /r/wallstreetbets/comments/o7vagy/weekend_disc...  1.624958e+09   \n",
       "\n",
       "             subreddit subreddit_id  \\\n",
       "0       wallstreetbets     t5_2th52   \n",
       "1       wallstreetbets     t5_2th52   \n",
       "2       wallstreetbets     t5_2th52   \n",
       "3       wallstreetbets     t5_2th52   \n",
       "4       wallstreetbets     t5_2th52   \n",
       "...                ...          ...   \n",
       "399863  wallstreetbets     t5_2th52   \n",
       "399864  wallstreetbets     t5_2th52   \n",
       "399865  wallstreetbets     t5_2th52   \n",
       "399866  wallstreetbets     t5_2th52   \n",
       "399867  wallstreetbets     t5_2th52   \n",
       "\n",
       "                                                     hash  \\\n",
       "0       77c0f32dcf506571815f3d4839454f2b3e550f0e1efecd...   \n",
       "1       b61949552c3a5d559111ba44d17a71113e408126dd198d...   \n",
       "2       97bf3c55f77337de3d6b83b05fc5601e2b346845b7b778...   \n",
       "3       5a3885658ca462a1fc0dd3b0af8858e183b05f4f474b84...   \n",
       "4       405314aad1e814100e822f7ac89274b17d66c4e11e5521...   \n",
       "...                                                   ...   \n",
       "399863  fdc96ffbf256523aec8846ae56321053c7ab751c99eb76...   \n",
       "399864  20c5bee31dc23c8937aea64670a4157c6547621f763ff4...   \n",
       "399865  96ac9390a4f9c8c36aeabf6d3f0358478ce7b3a295c9b6...   \n",
       "399866  6fa28767c5b75399225dd168a981c8570422fab812afab...   \n",
       "399867  5a9d2e286476b7403cb72369dd669ab680d5380add5ffd...   \n",
       "\n",
       "                                                    clean  \n",
       "0       [will, accept, payments, my, research, whats, ...  \n",
       "1       [previously, post, someone, claimed, looking, ...  \n",
       "2       [thought, was, going, give, research, time, aw...  \n",
       "3       [would, also, see, proof, back, claims, your, ...  \n",
       "4       [because, post, chart, their, site, do, includ...  \n",
       "...                                                   ...  \n",
       "399863                                             [nice]  \n",
       "399864                             [ah, horoscopes, real]  \n",
       "399865              [one, this, would, tell, fuck, outta]  \n",
       "399866  [canada, still, stupid, covid, restrictions, p...  \n",
       "399867  [go, fuck, with, cactus, the, weekend, fuck, t...  \n",
       "\n",
       "[399868 rows x 16 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#//*** Load Clean and Prepare data for aggregation\n",
    "start_time = time.time()\n",
    "print(\"Reading Compressed CSV\")\n",
    "raw_df = pd.read_csv(f\".\\\\data\\\\wallstreetbets_comments.csv.zip\", )\n",
    "print(f\"File Loaded: {round(time.time()-start_time,2)}s\")\n",
    "\n",
    "#//*** Convert UTC to date (not datetime)\n",
    "#//** Second pass goes from 12-21 to 4-19\n",
    "try:\n",
    "    raw_df['created_utc'] = raw_df['created_utc'].apply(lambda x: date.fromtimestamp(x))\n",
    "except:\n",
    "    print()\n",
    "\n",
    "\n",
    "raw_df['clean'] = remove_stop_words(tokenize_series(mr_clean_text(raw_df['body'],{\"remove_empty\":False})))\n",
    "\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Cleaning\n",
      "remove_empty False\n",
      "Text Cleaning Time: 4.3419764041900635\n",
      "Tokenize Time: 47.86150026321411\n",
      "Stop Words Time: 7.036121129989624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>is_submitter</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>body</th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>stickied</th>\n",
       "      <th>permalink</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>hash</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-08-24</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>I will accept payments for my research...what'...</td>\n",
       "      <td>c5y9v5z</td>\n",
       "      <td>t3_yqtpn</td>\n",
       "      <td>t1_c5y49wz</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1.429727e+09</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>77c0f32dcf506571815f3d4839454f2b3e550f0e1efecd...</td>\n",
       "      <td>[will, accept, payments, my, research, whats, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-08-24</td>\n",
       "      <td>False</td>\n",
       "      <td>t2_4p1mf</td>\n",
       "      <td>Because previously (until this post), when som...</td>\n",
       "      <td>c5yaaki</td>\n",
       "      <td>t3_yqtpn</td>\n",
       "      <td>t1_c5y9v5z</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1.429728e+09</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>b61949552c3a5d559111ba44d17a71113e408126dd198d...</td>\n",
       "      <td>[previously, post, someone, claimed, looking, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-08-24</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>So you thought I was just going to give all of...</td>\n",
       "      <td>c5yahuo</td>\n",
       "      <td>t3_yqtpn</td>\n",
       "      <td>t1_c5yaaki</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1.429728e+09</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>97bf3c55f77337de3d6b83b05fc5601e2b346845b7b778...</td>\n",
       "      <td>[thought, was, going, give, research, time, aw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-08-24</td>\n",
       "      <td>False</td>\n",
       "      <td>t2_3o5bc</td>\n",
       "      <td>I would also see some proof to back up your cl...</td>\n",
       "      <td>c5yaloj</td>\n",
       "      <td>t3_yqtpn</td>\n",
       "      <td>t1_c5y7jem</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1.429728e+09</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>5a3885658ca462a1fc0dd3b0af8858e183b05f4f474b84...</td>\n",
       "      <td>[would, also, see, proof, back, claims, your, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-08-24</td>\n",
       "      <td>False</td>\n",
       "      <td>t2_4p1mf</td>\n",
       "      <td>&amp;gt; So you thought I was just going to give a...</td>\n",
       "      <td>c5yamfs</td>\n",
       "      <td>t3_yqtpn</td>\n",
       "      <td>t1_c5yahuo</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1.429728e+09</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>405314aad1e814100e822f7ac89274b17d66c4e11e5521...</td>\n",
       "      <td>[because, post, chart, their, site, do, includ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399863</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-06-25</td>\n",
       "      <td>False</td>\n",
       "      <td>t2_9z0opsh3</td>\n",
       "      <td>Nice</td>\n",
       "      <td>h32dw7m</td>\n",
       "      <td>t3_o7jx7p</td>\n",
       "      <td>t1_h329ycg</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/o7jx7p/fraternal_as...</td>\n",
       "      <td>1.624958e+09</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>fdc96ffbf256523aec8846ae56321053c7ab751c99eb76...</td>\n",
       "      <td>[nice]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399864</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-06-25</td>\n",
       "      <td>False</td>\n",
       "      <td>t2_qc6iq</td>\n",
       "      <td>Ah so horoscopes ARE real</td>\n",
       "      <td>h32dwab</td>\n",
       "      <td>t3_o7z71d</td>\n",
       "      <td>t3_o7z71d</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/o7z71d/amc_what_hap...</td>\n",
       "      <td>1.624958e+09</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>20c5bee31dc23c8937aea64670a4157c6547621f763ff4...</td>\n",
       "      <td>[ah, horoscopes, real]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399865</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-06-25</td>\n",
       "      <td>False</td>\n",
       "      <td>t2_bl7b6dkh</td>\n",
       "      <td>If one did this I would tell them Fuck Outta Here</td>\n",
       "      <td>h32dwip</td>\n",
       "      <td>t3_o7vagy</td>\n",
       "      <td>t1_h32daow</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/o7vagy/weekend_disc...</td>\n",
       "      <td>1.624958e+09</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>96ac9390a4f9c8c36aeabf6d3f0358478ce7b3a295c9b6...</td>\n",
       "      <td>[one, this, would, tell, fuck, outta]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399866</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-06-25</td>\n",
       "      <td>False</td>\n",
       "      <td>t2_5b0a37kn</td>\n",
       "      <td>Canada has still those stupid Covid restrictio...</td>\n",
       "      <td>h32dwhz</td>\n",
       "      <td>t3_o7vagy</td>\n",
       "      <td>t3_o7vagy</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/o7vagy/weekend_disc...</td>\n",
       "      <td>1.624958e+09</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>6fa28767c5b75399225dd168a981c8570422fab812afab...</td>\n",
       "      <td>[canada, still, stupid, covid, restrictions, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399867</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-06-25</td>\n",
       "      <td>False</td>\n",
       "      <td>t2_f7cphdq</td>\n",
       "      <td>Go fuck yourself with a cactus. It’s the weeke...</td>\n",
       "      <td>h32dwt5</td>\n",
       "      <td>t3_o7vagy</td>\n",
       "      <td>t1_h32dsig</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/wallstreetbets/comments/o7vagy/weekend_disc...</td>\n",
       "      <td>1.624958e+09</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>5a9d2e286476b7403cb72369dd669ab680d5380add5ffd...</td>\n",
       "      <td>[go, fuck, with, cactus, the, weekend, fuck, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>399868 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        score  total_awards_received created_utc  is_submitter  \\\n",
       "0           1                    0.0  2012-08-24         False   \n",
       "1           3                    0.0  2012-08-24         False   \n",
       "2          -2                    0.0  2012-08-24         False   \n",
       "3           2                    0.0  2012-08-24         False   \n",
       "4           2                    0.0  2012-08-24         False   \n",
       "...       ...                    ...         ...           ...   \n",
       "399863      3                    0.0  2021-06-25         False   \n",
       "399864      3                    0.0  2021-06-25         False   \n",
       "399865      3                    0.0  2021-06-25         False   \n",
       "399866      4                    0.0  2021-06-25         False   \n",
       "399867      6                    0.0  2021-06-25         False   \n",
       "\n",
       "       author_fullname                                               body  \\\n",
       "0                    0  I will accept payments for my research...what'...   \n",
       "1             t2_4p1mf  Because previously (until this post), when som...   \n",
       "2                    0  So you thought I was just going to give all of...   \n",
       "3             t2_3o5bc  I would also see some proof to back up your cl...   \n",
       "4             t2_4p1mf  &gt; So you thought I was just going to give a...   \n",
       "...                ...                                                ...   \n",
       "399863     t2_9z0opsh3                                               Nice   \n",
       "399864        t2_qc6iq                          Ah so horoscopes ARE real   \n",
       "399865     t2_bl7b6dkh  If one did this I would tell them Fuck Outta Here   \n",
       "399866     t2_5b0a37kn  Canada has still those stupid Covid restrictio...   \n",
       "399867      t2_f7cphdq  Go fuck yourself with a cactus. It’s the weeke...   \n",
       "\n",
       "             id    link_id   parent_id  stickied  \\\n",
       "0       c5y9v5z   t3_yqtpn  t1_c5y49wz     False   \n",
       "1       c5yaaki   t3_yqtpn  t1_c5y9v5z     False   \n",
       "2       c5yahuo   t3_yqtpn  t1_c5yaaki     False   \n",
       "3       c5yaloj   t3_yqtpn  t1_c5y7jem     False   \n",
       "4       c5yamfs   t3_yqtpn  t1_c5yahuo     False   \n",
       "...         ...        ...         ...       ...   \n",
       "399863  h32dw7m  t3_o7jx7p  t1_h329ycg     False   \n",
       "399864  h32dwab  t3_o7z71d   t3_o7z71d     False   \n",
       "399865  h32dwip  t3_o7vagy  t1_h32daow     False   \n",
       "399866  h32dwhz  t3_o7vagy   t3_o7vagy     False   \n",
       "399867  h32dwt5  t3_o7vagy  t1_h32dsig     False   \n",
       "\n",
       "                                                permalink  retrieved_on  \\\n",
       "0                                                       0  1.429727e+09   \n",
       "1                                                       0  1.429728e+09   \n",
       "2                                                       0  1.429728e+09   \n",
       "3                                                       0  1.429728e+09   \n",
       "4                                                       0  1.429728e+09   \n",
       "...                                                   ...           ...   \n",
       "399863  /r/wallstreetbets/comments/o7jx7p/fraternal_as...  1.624958e+09   \n",
       "399864  /r/wallstreetbets/comments/o7z71d/amc_what_hap...  1.624958e+09   \n",
       "399865  /r/wallstreetbets/comments/o7vagy/weekend_disc...  1.624958e+09   \n",
       "399866  /r/wallstreetbets/comments/o7vagy/weekend_disc...  1.624958e+09   \n",
       "399867  /r/wallstreetbets/comments/o7vagy/weekend_disc...  1.624958e+09   \n",
       "\n",
       "             subreddit subreddit_id  \\\n",
       "0       wallstreetbets     t5_2th52   \n",
       "1       wallstreetbets     t5_2th52   \n",
       "2       wallstreetbets     t5_2th52   \n",
       "3       wallstreetbets     t5_2th52   \n",
       "4       wallstreetbets     t5_2th52   \n",
       "...                ...          ...   \n",
       "399863  wallstreetbets     t5_2th52   \n",
       "399864  wallstreetbets     t5_2th52   \n",
       "399865  wallstreetbets     t5_2th52   \n",
       "399866  wallstreetbets     t5_2th52   \n",
       "399867  wallstreetbets     t5_2th52   \n",
       "\n",
       "                                                     hash  \\\n",
       "0       77c0f32dcf506571815f3d4839454f2b3e550f0e1efecd...   \n",
       "1       b61949552c3a5d559111ba44d17a71113e408126dd198d...   \n",
       "2       97bf3c55f77337de3d6b83b05fc5601e2b346845b7b778...   \n",
       "3       5a3885658ca462a1fc0dd3b0af8858e183b05f4f474b84...   \n",
       "4       405314aad1e814100e822f7ac89274b17d66c4e11e5521...   \n",
       "...                                                   ...   \n",
       "399863  fdc96ffbf256523aec8846ae56321053c7ab751c99eb76...   \n",
       "399864  20c5bee31dc23c8937aea64670a4157c6547621f763ff4...   \n",
       "399865  96ac9390a4f9c8c36aeabf6d3f0358478ce7b3a295c9b6...   \n",
       "399866  6fa28767c5b75399225dd168a981c8570422fab812afab...   \n",
       "399867  5a9d2e286476b7403cb72369dd669ab680d5380add5ffd...   \n",
       "\n",
       "                                                    clean  \n",
       "0       [will, accept, payments, my, research, whats, ...  \n",
       "1       [previously, post, someone, claimed, looking, ...  \n",
       "2       [thought, was, going, give, research, time, aw...  \n",
       "3       [would, also, see, proof, back, claims, your, ...  \n",
       "4       [because, post, chart, their, site, do, includ...  \n",
       "...                                                   ...  \n",
       "399863                                             [nice]  \n",
       "399864                             [ah, horoscopes, real]  \n",
       "399865              [one, this, would, tell, fuck, outta]  \n",
       "399866  [canada, still, stupid, covid, restrictions, p...  \n",
       "399867  [go, fuck, with, cactus, the, weekend, fuck, t...  \n",
       "\n",
       "[399868 rows x 16 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#//*** Encodes the dataframe with a count of Ticker symbols in each comment.\n",
    "#//*** Called from update_subreddit(). This is broken out since we will likely need to adjust encoding parameters\n",
    "def aggregate_comments(raw_df):\n",
    "    import time\n",
    "\n",
    "    #print(\"Begin Cleaning\")\n",
    "\n",
    "    #//*** Clean text, tokenize and remove stop words\n",
    "    #raw_df['clean'] = remove_stop_words(tokenize_series(mr_clean_text(raw_df['body'],{\"remove_empty\":False})))\n",
    "\n",
    "\n",
    "    #//*** Stock Ticker Symbols to track\n",
    "    symbols = [\"CLOV\",\"SOFI\",\"WKHS\",\"AMD\",\"GME\",\"X\",\"AMC\",\"CLNE\",\"NIO\",\"MU\",\"SPCE\",\"BB\"]\n",
    "\n",
    "    #//*** Count each Stock mention add it to a dictionary of lists. Each list is filled with 0s. The Specific row index is updated with the relevant count. \n",
    "    #//*** This Generates a word count matrix\n",
    "    stock_dict = {}\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #//*** Group \n",
    "    for group in raw_df.groupby('created_utc'):\n",
    "        print(len(group[1]))\n",
    "        loop_df = group[1].copy()\n",
    "        \n",
    "        #//*** Initialze Stock Dict \n",
    "        for symbol in symbols:\n",
    "            stock_dict[symbol] = 0\n",
    "        \n",
    "\n",
    "\n",
    "        break\n",
    "\n",
    "#//*** Initialize Agregated DataFrame\n",
    "df = aggregate_comments(raw_df)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_empty False\n",
      "Text Cleaning Time: 4.485838174819946\n",
      "Tokenize Time: 49.1232545375824\n",
      "Stop Words Time: 7.268815994262695\n"
     ]
    }
   ],
   "source": [
    "raw_df['clean'] = remove_stop_words(tokenize_series(mr_clean_text(raw_df['body'],{\"remove_empty\":False})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Original Encode Comments, Keeping for reference\n",
    "#//*** Encodes the dataframe with a count of Ticker symbols in each comment.\n",
    "#//*** Called from update_subreddit(). This is broken out since we will likely need to adjust encoding parameters\n",
    "def encode_comments(raw_df):\n",
    "    import time\n",
    "    \n",
    "    print(\"Begin dataframe ticker symbol coding\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Build list of nasdaq and NYSE ticker symbols\n",
    "    #//*** Reads from Excel file.\n",
    "    #//*** Gets the Symbol column, and converts to lower case, \n",
    "    nyse = pd.read_csv(\"NYSE_20210625.csv\",header=None)[0].str.lower()\n",
    "    nasdaq = pd.read_csv(\"NASDAQ_20210625.csv\",header=None)[0].str.lower()\n",
    "\n",
    "    #//*** Removes symbols with 1 and 2 character listings\n",
    "    nyse = list(nyse[nyse.apply(lambda x: len(x)>2) ])\n",
    "    nasdaq = list(nasdaq[nasdaq.apply(lambda x: len(x)>2) ])\n",
    "\n",
    "    #//*** Combines both lists\n",
    "    symbols = nyse + nasdaq\n",
    "    \n",
    "\n",
    "    #//*** Count each Stock mention add it to a dictionary of lists. Each list is filled with 0s. The Specific row index is updated with the relevant count. \n",
    "    #//*** This Generates a word count matrix\n",
    "    stock_dict = {}\n",
    "\n",
    "    #//*** Keep Track of Rows\n",
    "    index = 0\n",
    "\n",
    "    for row in raw_df.iterrows():\n",
    "\n",
    "        #//*** Get the cleaned body text\n",
    "        body = row[1]['clean']\n",
    "\n",
    "        #//*** For Each Stock Symbol\n",
    "        for stock in symbols:\n",
    "\n",
    "            #//*** Check if Stock exists in Body\n",
    "            if stock in body:\n",
    "\n",
    "                #//*** Reset the stock counter\n",
    "                count = 0\n",
    "\n",
    "                #//*** Loop through body and county ticker mentions\n",
    "                for word in body:\n",
    "                    #//*** If word found increment count\n",
    "                    if stock == word:\n",
    "                        count += 1\n",
    "\n",
    "                #//*** Check if symbol is in stock_dict\n",
    "                if stock not in stock_dict.keys():    \n",
    "\n",
    "                    #//*** If not, then build it\n",
    "                    stock_dict[stock] = np.zeros(len(raw_df))\n",
    "\n",
    "                #//*** Update the stock value at the \n",
    "                stock_dict[stock][index] = count\n",
    "\n",
    "        #//*** Increment Index to keep with row index\n",
    "        index +=1   \n",
    "\n",
    "    #//*** Loop through the dictionary key and lists\n",
    "    for col,values in stock_dict.items():\n",
    "\n",
    "        #//*** Add each key (which is a stock ticker symbol) as a column using the list of ticker counts for Data\n",
    "        raw_df[col] = values.astype('int')\n",
    "\n",
    "    print(f\"Encoding Time: {round(time.time()-start_time,2)}s\")\n",
    "    \n",
    "    return raw_df\n",
    "\n",
    "#//*** Initialize Agregated DataFrame\n",
    "df = pd.DataFrame()\n",
    "\n",
    "#//*** Group \n",
    "for group in raw_df.groupby('created_utc'):\n",
    "    print(len(group[1]))\n",
    "    loop_df = group[1].copy()\n",
    "    \n",
    "    #//*** Clean text, tokenize and remove stop words\n",
    "    loop_df['clean'] = remove_stop_words(tokenize_series(mr_clean_text(loop_df['body'],{\"remove_empty\":False})))\n",
    "    \n",
    "    #//*** encode the comments\n",
    "    #//*** Breaking this out into a separate function for readability and possible future flexibility\n",
    "    loop_df = encode_comments(loop_df)\n",
    "    \n",
    "    break\n",
    "    \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
