{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import platform\n",
    "\n",
    "import stoneburner\n",
    "#//*** Custom Functions:\n",
    "#//*** mr_clean_text(input_series)\n",
    "#//*** tokenize_series(input_series)\n",
    "#//*** remove_stop_words(input_series)\n",
    "\n",
    "# //*** Imports and Load Data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#//*** Use the whole window in the IPYNB editor\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "subreddits = [\"wallstreetbets\", \"stocks\", \"wallstreetbetsOGs\", \"spacs\", \"investing\", \"pennystocks\", \"stockmarket\", \"options\", \"robinhoodpennystocks\", \"wallstreetbetsnew\", \"smallstreetbets\"]\n",
    "filepath = \".\\\\data\\\\\"\n",
    "filename_suffix = \"_comments.csv.zip\"\n",
    "#//*** Maximize columns and rows displayed by pandas\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clov', 'sofi', 'wkhs', 'amd', 'gme', 'x', 'amc', 'clne', 'nio', 'mu', 'spce', 'bb']\n",
      "Reading Compressed CSV: .\\data\\wallstreetbets_comments.csv.zip\n",
      "Reading Compressed CSV: .\\data\\stocks_comments.csv.zip\n",
      "Reading Compressed CSV: .\\data\\wallstreetbetsOGs_comments.csv.zip\n",
      "Reading Compressed CSV: .\\data\\spacs_comments.csv.zip\n",
      "Reading Compressed CSV: .\\data\\investing_comments.csv.zip\n",
      "Reading Compressed CSV: .\\data\\pennystocks_comments.csv.zip\n",
      "Reading Compressed CSV: .\\data\\stockmarket_comments.csv.zip\n",
      "Reading Compressed CSV: .\\data\\options_comments.csv.zip\n",
      "Reading Compressed CSV: .\\data\\robinhoodpennystocks_comments.csv.zip\n",
      "Reading Compressed CSV: .\\data\\wallstreetbetsnew_comments.csv.zip\n",
      "Reading Compressed CSV: .\\data\\smallstreetbets_comments.csv.zip\n",
      "Files Loaded: 51.33s\n",
      "Total Records: 4432533\n"
     ]
    }
   ],
   "source": [
    "#//*** Input_filename: Comments to Process.\n",
    "#//*** This will eventually be a list of files\n",
    "#input_filename  =\".\\\\data\\\\wallstreetbets_comments.csv.zip\"\n",
    "\n",
    "#//*** Path to processed files\n",
    "output_filename = \".\\\\data\\\\processed_reddit_basic_v3.csv.zip\"\n",
    "\n",
    "#//*** Path to the stock ticker JSON file\n",
    "stock_ticker_filename = \".\\\\data\\\\stock_tickers.json\"\n",
    "\n",
    "#//*** Convert Path to Mac formatting if needed\n",
    "if platform.system() == 'Darwin':\n",
    "    output_filename = output_filename.replace(\"\\\\\",\"/\")\n",
    "    stock_ticker_filename = stock_ticker_filename.replace(\"\\\\\",\"/\")\n",
    "\n",
    "#//*** Load the Stock Tickers\n",
    "f = open(stock_ticker_filename, \"r\")\n",
    "symbols = json.loads(f.read())['symbols']\n",
    "f.close()\n",
    "\n",
    "process_tfidf = False\n",
    "\n",
    "print(symbols)\n",
    "#//*** Convert symbols to lower case\n",
    "symbols = [x.lower() for x in symbols]\n",
    "\n",
    "\n",
    "\n",
    "raw_df = pd.DataFrame()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#//*** Load each Subreddit for Aggregation\n",
    "for subreddit in subreddits:\n",
    "    #//*** Filepath + subreddit name + csv.zip\n",
    "    input_filename = filepath+subreddit+filename_suffix\n",
    "\n",
    "    #//*** Convert Path to Mac formatting if needed\n",
    "    if platform.system() == 'Darwin':\n",
    "        input_filename = input_filename.replace(\"\\\\\",\"/\")\n",
    "   \n",
    "    print(f\"Reading Compressed CSV: {input_filename}\")\n",
    "    \n",
    "    #//*** Read Each DataFrame and combine with raw_df\n",
    "    raw_df = pd.concat([raw_df,pd.read_csv(input_filename,compression='zip' )])\n",
    "\n",
    "#//*** Reset the index, since multiple indexes have been combined\n",
    "raw_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Files Loaded: {round(time.time()-start_time,2)}s\")\n",
    "print(f\"Total Records: {len(raw_df)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_empty False\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 67.6 MiB for an array with shape (4432533,) and data type complex128",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c9f136bd2f38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#//*** Clean the Body Text, Tokenize and Remove Stop Words.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#//*************************************************************************\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mraw_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstoneburner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove_stop_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstoneburner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstoneburner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmr_clean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"remove_empty\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mprocess_tfidf\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DSCProjects\\DSC630_TermProject\\stoneburner.py\u001b[0m in \u001b[0;36mmr_clean_text\u001b[1;34m(input_series, input_options)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m#//*** Let's use regex to remove html entities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maction_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"html\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0minput_series\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_series\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'&.*;'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;31m#//*** Remove the empty lines\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\strings.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1952\u001b[0m                 )\n\u001b[0;32m   1953\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1954\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1956\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\strings.py\u001b[0m in \u001b[0;36mreplace\u001b[1;34m(self, pat, repl, n, case, flags, regex)\u001b[0m\n\u001b[0;32m   2774\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mforbid_nonstring_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"bytes\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2775\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2776\u001b[1;33m         result = str_replace(\n\u001b[0m\u001b[0;32m   2777\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2778\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\strings.py\u001b[0m in \u001b[0;36mstr_replace\u001b[1;34m(arr, pat, repl, n, case, flags, regex)\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 726\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_na_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\strings.py\u001b[0m in \u001b[0;36m_na_map\u001b[1;34m(f, arr, na_result, dtype)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mna_result\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mna_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_map_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\strings.py\u001b[0m in \u001b[0;36m_map_object\u001b[1;34m(f, arr, na_mask, na_value, dtype)\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[0mconvert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[1;31m# Reraise the exception if callable `f` got wrong number of args.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer_mask\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 67.6 MiB for an array with shape (4432533,) and data type complex128"
     ]
    }
   ],
   "source": [
    "#raw_df[raw_df['body'].str.len() == 0]\n",
    "\n",
    "raw_df['body'] = raw_df['body'].astype('str')\n",
    "\n",
    "#//*** Convert UTC to date (not datetime)\n",
    "#//** Second pass goes from 12-21 to 4-19\n",
    "try:\n",
    "    raw_df['created_utc'] = raw_df['created_utc'].apply(lambda x: date.fromtimestamp(x))\n",
    "except:\n",
    "    print()\n",
    "\n",
    "#//*************************************************************************\n",
    "#//*** Clean the Body Text, Tokenize and Remove Stop Words.\n",
    "#//*************************************************************************\n",
    "raw_df['clean'] = stoneburner.remove_stop_words(stoneburner.tokenize_series(stoneburner.mr_clean_text(raw_df['body'],{\"remove_empty\":False})))\n",
    "\n",
    "if process_tfidf == True:\n",
    "    #//*** Detokenize the clean column as tfidf\n",
    "    raw_df['tfidf'] = raw_df['clean'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_df['clean'].apply(x lambda for y in x: y.join(x) + \" \" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#//*************************************************************\n",
    "#//*** Load the Encode_comments Function\n",
    "#//*** Counts the Stock mentions in each Post.\n",
    "#//*** Adds the stock as a column to the Dataframe\n",
    "#//*************************************************************\n",
    "\n",
    "def encode_comments(input_df):\n",
    "    import time\n",
    "    \n",
    "    print(\"Begin dataframe ticker symbol coding\")\n",
    "    start_time = time.time()\n",
    "       \n",
    "    \n",
    "    \n",
    "    #//*** Count each Stock mention add it to a dictionary of lists. Each list is filled with 0s. The Specific row index is updated with the relevant count. \n",
    "    #//*** This Generates a word count matrix\n",
    "    stock_dict = {}\n",
    "\n",
    "    #//*** Keep Track of Rows\n",
    "    index = 0\n",
    "\n",
    "    for row in input_df.iterrows():\n",
    "\n",
    "        #//*** Get the cleaned body text\n",
    "        body = row[1]['clean']\n",
    "\n",
    "        #//*** For Each Stock Symbol\n",
    "        for stock in symbols:\n",
    "            \n",
    "            #//*** Check if Stock exists in Body\n",
    "            if stock in body:\n",
    "\n",
    "                #//*** Reset the stock counter\n",
    "                count = 0\n",
    "\n",
    "                #//*** Loop through body and county ticker mentions\n",
    "                for word in body:\n",
    "                    #//*** If word found increment count\n",
    "                    if stock == word:\n",
    "                        count += 1\n",
    "\n",
    "                #//*** Check if symbol is in stock_dict\n",
    "                if stock not in stock_dict.keys():    \n",
    "\n",
    "                    #//*** If not, then build it\n",
    "                    stock_dict[stock] = np.zeros(len(raw_df))\n",
    "\n",
    "                #//*** Update the stock value at the \n",
    "                stock_dict[stock][index] = count\n",
    "\n",
    "        #//*** Increment Index to keep with row index\n",
    "        index +=1   \n",
    "\n",
    "    #//*** Loop through the dictionary key and lists\n",
    "    for col,values in stock_dict.items():\n",
    "\n",
    "        #//*** Add each key (which is a stock ticker symbol) as a column using the list of ticker counts for Data\n",
    "        raw_df[col] = values.astype('int')\n",
    "\n",
    "    print(f\"Encoding Time: {round(time.time()-start_time,2)}s\")\n",
    "    \n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Encodes the dataframe with a count of Ticker symbols in each comment.\n",
    "#//*** Called from update_subreddit(). This is broken out since we will likely need to adjust encoding parameters\n",
    "def aggregate_comments(input_df,is_tfidf):\n",
    "    \n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "    to_sum_cols = ['score','total_awards_received']\n",
    "    to_count_col = ['author_fullname','link_id']\n",
    "    \n",
    "    \n",
    "    df_cols = ['date','total_posts','tfidf']\n",
    "    \n",
    "    rename_cols = {\n",
    "        'total_awards_received' : 'awards',\n",
    "        'author_fullname' : 'authors',\n",
    "        'link_id' : 'threads'\n",
    "    }\n",
    "    \n",
    "    #//*** Build the OUtput Dataframe Column names from the Columns to sum, the columns to count, and the stock ticker columns\n",
    "    #//*** Loop through each list\n",
    "    for cols in [ to_sum_cols, to_count_col, symbols ]:\n",
    "        \n",
    "        #//*** Get individual column name from each column list\n",
    "        for col in cols:\n",
    "            print\n",
    "            #//*** Rename the column if in rename_col\n",
    "            #//*** Add col to df_cols....The out_df column names\n",
    "            if col in rename_cols.keys():\n",
    "                df_cols.append(rename_cols[col])\n",
    "            else:\n",
    "                df_cols.append(col)\n",
    "                \n",
    "    print(df_cols)\n",
    "    \n",
    "    out_df = pd.DataFrame(columns = df_cols)\n",
    "    \n",
    "   \n",
    "    #//*** Group \n",
    "    for group in input_df.groupby('created_utc'):\n",
    "        #//*** Start Timing the process\n",
    "        start_time = time.time()\n",
    "\n",
    "        loop_df = group[1].copy()\n",
    "        \n",
    "        loop_list = []\n",
    "\n",
    "        \n",
    "        #//*** Build the aggregated row for the Dataframe.\n",
    "        #//*** 5 Parts: \n",
    "        #//******** 1.) Date & Total Posts\n",
    "        #//******** 2.) tfidf - Bag of Words for the Day\n",
    "        #//******** 2.) Columns to sum\n",
    "        #//******** 3.) Columns to count\n",
    "        #//******** 4.) Stock Ticker columns to sum\n",
    "        \n",
    "        #//********************************************\n",
    "        #//******** 1.) Date & Total Posts\n",
    "        #//********************************************\n",
    "        #//*** Add the Date\n",
    "        loop_list.append(group[0])\n",
    "        \n",
    "        #//*** Add Total number of posts\n",
    "        loop_list.append(len(loop_df))\n",
    "        \n",
    "        #//********************************************\n",
    "        #//******** 2.) Build tfidf\n",
    "        #//********************************************\n",
    "        \n",
    "        \n",
    "        \n",
    "        #//*** Initialize the Vectorizer\n",
    "        tfidf = TfidfVectorizer()\n",
    "\n",
    "        #//*** Build the feature matrix, which is a weighted sparse matrix\n",
    "        loop_list.append(tfidf.fit_transform(input_df['tfidf']))\n",
    "        \n",
    "        #//********************************************\n",
    "        #//******** 2.) Columns to sum\n",
    "        #//********************************************\n",
    "        for col in to_sum_cols:\n",
    "            loop_list.append(loop_df[col].sum())\n",
    "\n",
    "            \n",
    "        #//********************************************\n",
    "        #//******** 3.) Columns to count\n",
    "        #//********************************************\n",
    "        for col in to_count_col:\n",
    "            loop_list.append(len(loop_df[col].unique()))\n",
    "    \n",
    "        \n",
    "        #//********************************************\n",
    "        #//******** 4.) Stock Ticker columns to sum\n",
    "        #//********************************************\n",
    "        for col in symbols:\n",
    "            loop_list.append(loop_df[col].sum())\n",
    "\n",
    "        #print(len(out_df.columns),len(loop_list))\n",
    "        #print(out_df.columns)\n",
    "        out_df.loc[len(out_df.index)] = loop_list \n",
    "        \n",
    "        print(f\"{group[0]} {len(loop_df)} Comments in {round(time.time() - start_time,2)}s\")\n",
    "    print(\"Aggregation Complete!\")\n",
    "    return out_df\n",
    "\n",
    "#for col in df.columns[16:]:\n",
    "#    print(df[df[col] > 0 ].iloc[0]['created_utc'],col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Encode Comments\n",
    "df = encode_comments(raw_df,process_tfidf)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Aggregate and Process Comments\n",
    "ag_df = aggregate_comments(df)\n",
    "ag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_filename = \".\\\\data\\\\processed_reddit_basic_v2.csv.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Write File to disk\n",
    "ag_df.to_csv(output_filename,compression=\"zip\",index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
