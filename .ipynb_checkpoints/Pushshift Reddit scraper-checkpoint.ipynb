{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/swlh/how-to-scrape-large-amounts-of-reddit-data-using-pushshift-1d33bde9286"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# //*** Imports and Load Data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "from pmaw import PushshiftAPI\n",
    "\n",
    "\n",
    "#//*** Use the whole window in the IPYNB editor\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#//*** Maximize columns and rows displayed by pandas\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "api = PushshiftAPI()\n",
    "\n",
    "import datetime as dt\n",
    "before = int(dt.datetime(2021,6,26,0,0).timestamp())\n",
    "after = int(dt.datetime(2020,12,1,0,0).timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//***************************************\n",
    "#//*** Apply Common Cleanup operations\n",
    "#//***************************************\n",
    "#//*** In anticpation that I'll be re-using text cleanup code. I'm adding some robustness to the function.\n",
    "#//*** Adding kwargs to disable features that default to true.\n",
    "#//*** Whether an action is skipped or executed is based on a boolean value stored in action_dict.\n",
    "#//*** Key values will default to true. If code needs to be defaulted to False, a default_false list can be added later\n",
    "#//*** All Boolean kwarg keya are stored in kwarg list. This speeds up the coding of the action_dict.\n",
    "#//*** As Kwargs are added \n",
    "def mr_clean_text(input_series, input_options={}):\n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "    \n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Add some data validation. I'm preparing this function for additional use. I'm checking if future users (ie future me)\n",
    "    #//*** may throw some garbage at this function. Experience has taught me to fail safely wherever possible.\n",
    "\n",
    "    #//*** All kwargs are listed here. These initialize TRUE by default.\n",
    "    key_list = [ \"lower\", \"newline\", \"html\", \"remove_empty\", \"punctuation\" ]\n",
    "    \n",
    "    #//*** Build Action Dictionary\n",
    "    action_dict = { } \n",
    "    \n",
    "    #//*** Build the keys from kwarg_list and default them to TRUE\n",
    "    for key in key_list:\n",
    "        action_dict[key] = True\n",
    "        \n",
    "    #//*** Loop through the input kwargs (if any). Assign the action_dict values based on the kwargs:\n",
    "    for key,value in input_options.items():\n",
    "        print(key,value)\n",
    "        action_dict[key] = value\n",
    "    \n",
    "    \n",
    "    #//*************************************************************************\n",
    "    #//*** The Cleanup/Processing code is a straight lift from DSC550 - Week02\n",
    "    #//*************************************************************************\n",
    "    #//*** Convert to Lower Case, Default to True\n",
    "    if action_dict[\"lower\"]:\n",
    "        input_series = input_series.str.lower()\n",
    "    \n",
    "   \n",
    "    #//*** Remove New Lines\n",
    "    if action_dict[\"newline\"]:\n",
    "        #//*** Rmove \\r\\n\n",
    "        input_series = input_series.str.replace(r'\\r?\\n',\"\")\n",
    "\n",
    "        #//*** Remove \\n new lines\n",
    "        input_series = input_series.str.replace(r'\\n',\"\")\n",
    "\n",
    "    #//*** Remove html entities, observed entities are &gt; and &lt;. All HTML entities begin with & and end with ;.\n",
    "    #//*** Let's use regex to remove html entities\n",
    "    if action_dict[\"html\"]:\n",
    "        input_series = input_series.str.replace(r'&.*;',\"\")\n",
    "\n",
    "    #//*** Remove the empty lines\n",
    "    if action_dict[\"remove_empty\"]:\n",
    "        input_series = input_series[ input_series.str.len() > 0]\n",
    "\n",
    "    #//*** Remove punctuation\n",
    "    if action_dict[\"punctuation\"]:\n",
    "        #//*** Load libraries for punctuation if not already loaded.\n",
    "        #//*** Wrapping these in a try, no sense in importing libraries that already exist.\n",
    "        #//*** Unsure of the cost of reimporting libraries (if any). But testing if library is already loaded feels\n",
    "        #//*** like a good practice\n",
    "        try:\n",
    "            type(sys)\n",
    "        except:\n",
    "            import sys\n",
    "\n",
    "        try:\n",
    "            type(unicodedata)\n",
    "        except:\n",
    "            import unicodedata\n",
    "        \n",
    "        #//*** replace Comma and Period with a space.\n",
    "        for punct in [\",\",\".\",\"$\"]:\n",
    "            input_series = input_series.str.replace(punct,\" \")\n",
    "\n",
    "        #//*** Remove punctuation using the example from the book\n",
    "        punctuation = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P') )\n",
    "        input_series = input_series.str.translate(punctuation)\n",
    "\n",
    "    print(f\"Text Cleaning Time: {time.time() - start_time}\")\n",
    "\n",
    "    return input_series\n",
    "#//*** Remove Stop words from the input list\n",
    "def remove_stop_words(input_series):\n",
    "    \n",
    "    #//*** This function removes stop_words from a series.\n",
    "    #//*** Works with series.apply()\n",
    "    def apply_stop_words(input_list):\n",
    "\n",
    "        #//*** Load Stopwords   \n",
    "        for word in input_list:\n",
    "            if word in stop_words:\n",
    "                input_list.remove(word)\n",
    "        return input_list\n",
    "\n",
    "    #//*** import nltk if needed\n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "        \n",
    "    stopwords = nltk.corpus.stopwords\n",
    "\n",
    "    #//*** Stopwords requires an additional download\n",
    "    try:\n",
    "        type(stopwords)\n",
    "    except:\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "\n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "\n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    #//*** The stop_words include punctuation. Stop Word Contractions will not be filtered out.\n",
    "    stop_words = []\n",
    "\n",
    "    #//*** Remove apostrophies from the stop_words\n",
    "    for stop in stopwords.words('english'):\n",
    "        stop_words.append(stop.replace(\"'\",\"\"))\n",
    "\n",
    "    \n",
    "    #//*** Remove Stop words from the tokenized strings in the 'process' column\n",
    "    #input_series = input_series.apply(remove_stop_words,stop_words)\n",
    "    \n",
    "    input_series = input_series.apply(apply_stop_words)\n",
    "\n",
    "    print(f\"Stop Words Time: {time.time() - start_time}\")\n",
    "    \n",
    "    return input_series\n",
    "#//*** Tokenize a Series containing Strings.\n",
    "#//*** Breaking this out into it's own function for later reuse.\n",
    "#//*** Not a lot of code here, but it helps to keep the libraries localized. This creates standarization for future\n",
    "#//*** Stoneburner projects. Also has the ability to add functionality as needed.\n",
    "\n",
    "def tokenize_series(input_series):\n",
    "    \n",
    "    try:\n",
    "        type(nltk)\n",
    "    except:\n",
    "        import nltk\n",
    "    \n",
    "    word_tokenize = nltk.tokenize.word_tokenize \n",
    "    \n",
    "    #//*** import time library\n",
    "    try:\n",
    "        type(time)\n",
    "    except:\n",
    "        import time\n",
    "    \n",
    "    #//*** Start Timing the process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    input_series = input_series.apply(word_tokenize)\n",
    "    \n",
    "    print(f\"Tokenize Time: {time.time() - start_time}\")\n",
    "    \n",
    "    return input_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Update a target subreddit with new data\n",
    "#//*** subreddit = subbreddit name\n",
    "#//*** Method = 'before' or 'after' indicating if the records to be retrieved are before or after the target_utc. Defaults to After\n",
    "#//*** limit is the number or records to grab\n",
    "def update_subreddit(subreddit,method,limit):\n",
    "    import time\n",
    "    filename = f\".\\\\data\\\\{subreddit}_comments.csv.zip\"\n",
    "    print(f\"Reading csv: {filename}\")\n",
    "    start_time = time.time()\n",
    "    update_df = pd.read_csv(filename, compression = 'zip')\n",
    "    \n",
    "    print(f\"csv loaded: {round(time.time()-start_time,2)}s\")\n",
    "    \n",
    "    print(f\"csv Record count: {len(update_df)}\")\n",
    "\n",
    "    #//*** If before, get the largest (latest) utc\n",
    "    if method == 'before':\n",
    "        #//*** Get the Before utc from the stored csv\n",
    "        before = update_df['created_utc'].max() \n",
    "        \n",
    "        print(f\"Getting {limit} records before {before} utc\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        #//*** Download comments\n",
    "        comments = api.search_comments(subreddit=subreddit, limit=limit, before=before)\n",
    "        \n",
    "        print(f\"Download Time: {round(time.time()-start_time,2)}s\")\n",
    "        \n",
    "    elif method == 'after':\n",
    "        after = update_df['created_utc'].min() \n",
    "        \n",
    "        print(f\"Getting {limit} records after {after} utc\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        #//*** Download comments\n",
    "        comments = api.search_comments(subreddit=subreddit, limit=limit, after=after)\n",
    "        print(f\"Download Time: {round(time.time()-start_time,2)}s\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Method needs to be 'before' or 'after': [{method}] is invalid\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    #//***************************************************************************\n",
    "    #//*** Download Complete\n",
    "    #//***************************************************************************\n",
    "\n",
    "    #//*** Convert comments to Dataframe\n",
    "    raw_df = pd.DataFrame(comments)\n",
    "    \n",
    "\n",
    "    #//*** Keep the important columns\n",
    "    raw_df = raw_df[[\"score\",\"total_awards_received\",\"created_utc\",\"is_submitter\",\"author_fullname\",\"body\",\"id\",\"link_id\",\"parent_id\",\"stickied\",\"permalink\",\"retrieved_on\",\"subreddit\",\"subreddit_id\"]]\n",
    "\n",
    "    print(f\"Checking For Duplicates - Length Before: {len(raw_df)}\")\n",
    "    \n",
    "    #//*** Hash the body, will use to check for duplicates\n",
    "    #//*** Hash the body using sha-256\n",
    "    #Sha256: Reference https://www.pythonpool.com/python-sha256/\n",
    "\n",
    "    raw_df['hash'] = raw_df['body'].apply(lambda x:hashlib.sha256(x.encode()).hexdigest())\n",
    "\n",
    "\n",
    "    # dropping Duplicates First. No sense in processing these\n",
    "    raw_df.drop_duplicates(subset =\"hash\",keep = False, inplace = True)\n",
    "    \n",
    "    print(f\"Checking For Duplicates - Length After: {len(raw_df)}\")\n",
    "\n",
    "    print(\"Begin Cleaning\")\n",
    "\n",
    "    #//*** Clean text, tokenize and remove stop words\n",
    "    raw_df['clean'] = remove_stop_words(tokenize_series(mr_clean_text(raw_df['body'],{\"remove_empty\":False})))\n",
    "    \n",
    "    #//*** encode the comments\n",
    "    #//*** Breaking this out into a separate function for readability and possible future flexibility\n",
    "    raw_df = encode_comments(raw_df)\n",
    "    \n",
    "    #//*** Combining existing dataframe with raw_df\n",
    "    update_df = pd.concat([update_df,raw_df])\n",
    "    print(f\"Combined Dataframe Size:{len(update_df)}\")\n",
    "\n",
    "    # Check for Duplicates. \n",
    "    update_df.drop_duplicates(subset =\"hash\",keep = False, inplace = True)\n",
    "    print(f\"Dropping Duplicates - New Size:{len(update_df)}\")\n",
    "\n",
    "    print(\"Replace NaN with Zeros\")\n",
    "    update_df = update_df.fillna(0)\n",
    "\n",
    "    print(f\"Writing {filename}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    update_df.to_csv(filename,compression=\"zip\",index=False)    \n",
    "    \n",
    "    print(f\"File Written: {round(time.time()-start_time,2)}s\")\n",
    "    \n",
    "    print(f\"update_subreddit() Complete: {len(update_df)} records\")\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Encodes the dataframe with a count of Ticker symbols in each comment.\n",
    "#//*** Called from update_subreddit(). This is broken out since we will likely need to adjust encoding parameters\n",
    "def encode_comments(raw_df):\n",
    "    import time\n",
    "    \n",
    "    print(\"Begin dataframe ticker symbol coding\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #//*** Build list of nasdaq and NYSE ticker symbols\n",
    "    #//*** Reads from Excel file.\n",
    "    #//*** Gets the Symbol column, and converts to lower case, \n",
    "    nyse = pd.read_csv(\"NYSE_20210625.csv\",header=None)[0].str.lower()\n",
    "    nasdaq = pd.read_csv(\"NASDAQ_20210625.csv\",header=None)[0].str.lower()\n",
    "\n",
    "    #//*** Removes symbols with 1 and 2 character listings\n",
    "    nyse = list(nyse[nyse.apply(lambda x: len(x)>2) ])\n",
    "    nasdaq = list(nasdaq[nasdaq.apply(lambda x: len(x)>2) ])\n",
    "\n",
    "    #//*** Combines both lists\n",
    "    symbols = nyse + nasdaq\n",
    "    \n",
    "\n",
    "    #//*** Count each Stock mention add it to a dictionary of lists. Each list is filled with 0s. The Specific row index is updated with the relevant count. \n",
    "    #//*** This Generates a word count matrix\n",
    "    stock_dict = {}\n",
    "\n",
    "    #//*** Keep Track of Rows\n",
    "    index = 0\n",
    "\n",
    "    for row in raw_df.iterrows():\n",
    "\n",
    "        #//*** Get the cleaned body text\n",
    "        body = row[1]['clean']\n",
    "\n",
    "        #//*** For Each Stock Symbol\n",
    "        for stock in symbols:\n",
    "\n",
    "            #//*** Check if Stock exists in Body\n",
    "            if stock in body:\n",
    "\n",
    "                #//*** Reset the stock counter\n",
    "                count = 0\n",
    "\n",
    "                #//*** Loop through body and county ticker mentions\n",
    "                for word in body:\n",
    "                    #//*** If word found increment count\n",
    "                    if stock == word:\n",
    "                        count += 1\n",
    "\n",
    "                #//*** Check if symbol is in stock_dict\n",
    "                if stock not in stock_dict.keys():    \n",
    "\n",
    "                    #//*** If not, then build it\n",
    "                    stock_dict[stock] = np.zeros(len(raw_df))\n",
    "\n",
    "                #//*** Update the stock value at the \n",
    "                stock_dict[stock][index] = count\n",
    "\n",
    "        #//*** Increment Index to keep with row index\n",
    "        index +=1   \n",
    "\n",
    "    #//*** Loop through the dictionary key and lists\n",
    "    for col,values in stock_dict.items():\n",
    "\n",
    "        #//*** Add each key (which is a stock ticker symbol) as a column using the list of ticker counts for Data\n",
    "        raw_df[col] = values.astype('int')\n",
    "\n",
    "    print(f\"Encoding Time: {round(time.time()-start_time,2)}s\")\n",
    "    \n",
    "    return raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading csv: .\\data\\wallstreetbets_comments.csv.zip\n",
      "csv loaded: 27.98s\n",
      "csv Record count: 150696\n",
      "Getting 100000 records before 1624634035 utc\n",
      "Total:: Success Rate: 100.00% - Requests: 1008 - Batches: 101 - Items Remaining: 0\n",
      "Download Time: 1022.24s\n",
      "Checking For Duplicates - Length Before: 100000\n",
      "Checking For Duplicates - Length After: 87007\n",
      "Begin Cleaning\n",
      "remove_empty False\n",
      "Text Cleaning Time: 1.3337349891662598\n",
      "Tokenize Time: 10.588387250900269\n",
      "Stop Words Time: 1.6063616275787354\n",
      "Begin dataframe ticker symbol coding\n",
      "Encoding Time: 133.39s\n",
      "Combined Dataframe Size:237703\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.50 GiB for an array with shape (1974, 237703) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-a95ddd1a35d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#//*** Add 100k Comments to wallstreetbets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mupdate_subreddit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"wallstreetbets\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"before\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-128-fc73a209d622>\u001b[0m in \u001b[0;36mupdate_subreddit\u001b[1;34m(subreddit, method, limit)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;31m# Check for Duplicates.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mupdate_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m\"hash\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Dropping Duplicates - New Size:{len(update_df)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop_duplicates\u001b[1;34m(self, subset, keep, inplace, ignore_index)\u001b[0m\n\u001b[0;32m   4813\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4814\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0minds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ndarray_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4815\u001b[1;33m             \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4817\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[0;32m   1375\u001b[0m         \u001b[0mTake\u001b[0m \u001b[0mitems\u001b[0m \u001b[0malong\u001b[0m \u001b[0many\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m         \"\"\"\n\u001b[1;32m-> 1377\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1378\u001b[0m         indexer = (\n\u001b[0;32m   1379\u001b[0m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"int64\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    943\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 945\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   1884\u001b[0m     \u001b[0mnew_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1886\u001b[1;33m         merged_blocks = _merge_blocks(\n\u001b[0m\u001b[0;32m   1887\u001b[0m             \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_can_consolidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1888\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, _can_consolidate)\u001b[0m\n\u001b[0;32m   3106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3107\u001b[0m         \u001b[0margsort\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3108\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3109\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.50 GiB for an array with shape (1974, 237703) and data type float64"
     ]
    }
   ],
   "source": [
    "#//*** Add 100k Comments to wallstreetbets\n",
    "update_subreddit(\"wallstreetbets\",\"before\",100000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150696\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_df['hash'].unique()))\n",
    "print(len(raw_df.tail()))\n",
    "#raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Reference to manually read & write to Data Frame\n",
    "#filename = f\".\\\\data\\\\{subreddit}_comments.csv.zip\"\n",
    "#update_df = pd.read_csv(filename, compression = 'zip')\n",
    "#print(len(update_df))\n",
    "#update_df\n",
    "#update_df.to_csv(filename,compression=\"zip\",index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_df.to_csv(\"reddit_comments.csv.zip\",compression=\"zip\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#//*** Push shift scraper reference\n",
    "\"\"\"\n",
    "#//*** Download the First 100,000 Comments from reddit pushsift\n",
    "subreddit=\"wallstreetbets\"\n",
    "limit=100\n",
    "#comments = api.search_comments(subreddit=subreddit, limit=limit, before=before after=after)\n",
    "comments = api.search_comments(subreddit=subreddit, limit=limit, after=after)\n",
    "print(f'Retrieved {len(comments)} comments from Pushshift')\n",
    "\n",
    "#//*** Convert comments to Dataframe\n",
    "comments_df = pd.DataFrame(comments)\n",
    "\n",
    "#//*** Save DataFrame to a file for processing\n",
    "comments_df.to_csv(f\"{subreddit}_raw_comments.csv.zip\",compression=\"zip\",index=False)\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#//*** Reference to create Stock ticker count matrix\n",
    "\"\"\"\n",
    "\n",
    "#//*** Build list of ticker symbols from NYSE and NASDAQ\n",
    "#//*** Reads from Excel file.\n",
    "#//*** Gets the Symbol column, and converts to lower case, \n",
    "nyse = pd.read_csv(\"NYSE_20210625.csv\",header=None)[0].str.lower()\n",
    "nasdaq = pd.read_csv(\"NASDAQ_20210625.csv\",header=None)[0].str.lower()\n",
    "\n",
    "#//*** Removes symbols with 1 and 2 character listings\n",
    "nyse = list(nyse[nyse.apply(lambda x: len(x)>2) ])\n",
    "nasdaq = list(nasdaq[nasdaq.apply(lambda x: len(x)>2) ])\n",
    "\n",
    "symbols = nyse + nasdaq\n",
    "\n",
    "#//*** Count each Stock mention add it to a dictionary of lists. Each list is filled with 0s. The Specific row index is updated with the relevant count. \n",
    "#//*** This Generates a word count matrix\n",
    "stock_dict = {}\n",
    "\n",
    "#//*** Keep Track of Rows\n",
    "index = 0\n",
    "\n",
    "for row in raw_df.iterrows():\n",
    "    \n",
    "    #//*** Get the cleaned body text\n",
    "    body = row[1]['clean']\n",
    "    \n",
    "    #//*** For Each Stock Symbol\n",
    "    for stock in symbols:\n",
    "        \n",
    "        #//*** Check if Stock exists in Body\n",
    "        if stock in body:\n",
    "            \n",
    "            #//*** Reset the stock counter\n",
    "            count = 0\n",
    "            \n",
    "            #//*** Loop through body and county ticker mentions\n",
    "            for word in body:\n",
    "                #//*** If word found increment count\n",
    "                if stock == word:\n",
    "                    count += 1\n",
    "                    \n",
    "            #//*** Check if symbol is in stock_dict\n",
    "            if stock not in stock_dict.keys():    \n",
    "\n",
    "                #//*** If not, then build it\n",
    "                stock_dict[stock] = np.zeros(len(raw_df))\n",
    "            \n",
    "            #//*** Update the stock value at the \n",
    "            stock_dict[stock][index] = count\n",
    "\n",
    "    #//*** Increment Index to keep with row index\n",
    "    index +=1   \n",
    "\n",
    "#//*** Loop through the dictionary key and lists\n",
    "for col,values in stock_dict.items():\n",
    "    \n",
    "    #//*** Add each key (which is a stock ticker symbol) as a column using the list of ticker counts for Data\n",
    "    raw_df[col] = values.astype('int')\n",
    "    \n",
    "\"\"\"\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
