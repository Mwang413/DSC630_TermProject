{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*************************************************************************\n",
    "#//*** Downloads Stock Data and converts the returns to a dataframe,\n",
    "#//*** which saves a compressed CSV file in the stocks folder\n",
    "#//*************************************************************************\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "#//*********************************************************************************\n",
    "#//*** Read the API keys from a JSON encoded file\n",
    "#//*** Located in the ignore_folder sub directory\n",
    "#//*** This Folder is added to the .gitignore file and does not show up on Github\n",
    "#//*** This is Authentication Best Practices for Github\n",
    "#//*********************************************************************************\n",
    "f = open(\"./ignore_folder/alpha_vantage_api.json\", \"r\")\n",
    "\n",
    "#//*** Fugley Pythonic type conversion\n",
    "#//*** Loads the file into Dictionary via JSON.loads\n",
    "#//*** Gets the API key value using the 'api' key\n",
    "#//*** prepends apikey= so the resulting value is URL ready :]\n",
    "av_apikey = json.loads(f.read())['apikey']\n",
    "f.close()\n",
    "\n",
    "#//*** Load the Stock Tickers\n",
    "f = open(\".\\\\data\\\\stock_tickers.json\", \"r\")\n",
    "symbols = json.loads(f.read())['symbols']\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building URL: clov\n",
      "Downloading....\n",
      "Processing....\n",
      "Building Dataframe\n",
      "Writing dataframe to File: .\\stocks\\clov_daily.csv.zip\n",
      "Waiting 20 Seconds\n",
      "Building URL: sofi\n",
      "Downloading....\n",
      "Processing....\n",
      "Building Dataframe\n",
      "Writing dataframe to File: .\\stocks\\sofi_daily.csv.zip\n",
      "Waiting 20 Seconds\n",
      "Building URL: wkhs\n",
      "Downloading....\n",
      "Processing....\n",
      "Building Dataframe\n",
      "Writing dataframe to File: .\\stocks\\wkhs_daily.csv.zip\n",
      "Waiting 20 Seconds\n",
      "Building URL: amd\n",
      "Downloading....\n",
      "Processing....\n",
      "Building Dataframe\n",
      "Writing dataframe to File: .\\stocks\\amd_daily.csv.zip\n",
      "Waiting 20 Seconds\n",
      "Building URL: gme\n",
      "Downloading....\n",
      "Processing....\n",
      "Building Dataframe\n",
      "Writing dataframe to File: .\\stocks\\gme_daily.csv.zip\n",
      "Waiting 20 Seconds\n",
      "Building URL: x\n",
      "Downloading....\n",
      "Processing....\n",
      "Building Dataframe\n",
      "Writing dataframe to File: .\\stocks\\x_daily.csv.zip\n",
      "Waiting 20 Seconds\n",
      "Building URL: amc\n",
      "Downloading....\n",
      "Processing....\n",
      "Building Dataframe\n",
      "Writing dataframe to File: .\\stocks\\amc_daily.csv.zip\n",
      "Waiting 20 Seconds\n",
      "Building URL: clne\n",
      "Downloading....\n",
      "Processing....\n",
      "Building Dataframe\n",
      "Writing dataframe to File: .\\stocks\\clne_daily.csv.zip\n",
      "Waiting 20 Seconds\n",
      "Building URL: nio\n",
      "Downloading....\n",
      "Processing....\n",
      "Building Dataframe\n",
      "Writing dataframe to File: .\\stocks\\nio_daily.csv.zip\n",
      "Waiting 20 Seconds\n",
      "Building URL: mu\n",
      "Downloading....\n",
      "Processing....\n",
      "Building Dataframe\n",
      "Writing dataframe to File: .\\stocks\\mu_daily.csv.zip\n",
      "Waiting 20 Seconds\n",
      "Building URL: spce\n",
      "Downloading....\n",
      "Processing....\n",
      "Building Dataframe\n",
      "Writing dataframe to File: .\\stocks\\spce_daily.csv.zip\n",
      "Waiting 20 Seconds\n",
      "Building URL: bb\n",
      "Downloading....\n",
      "Processing....\n",
      "Building Dataframe\n",
      "Writing dataframe to File: .\\stocks\\bb_daily.csv.zip\n",
      "Waiting 20 Seconds\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "#//*** Alpha Vantage API Docs:\n",
    "#//*** https://www.alphavantage.co/documentation/\n",
    "\n",
    "#//*** Intra day Query\n",
    "#symbol = \"amc\"\n",
    "#url = f'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol={symbol}&interval=5min&apikey={av_apikey}'\n",
    "\n",
    "#//*** get prices throughout today\n",
    "#action = \"TIME_SERIES_INTRADAY\"\n",
    "#action = \"TIME_SERIES_DAILY\"\n",
    "#//*** Intraday prices going back two yeares\n",
    "#action = \"TIME_SERIES_INTRADAY_EXTENDED\"\n",
    "#url = f'https://www.alphavantage.co/query?function={action}&symbol={symbol}&interval=60min&slice=year1month1&apikey={av_apikey}'\n",
    "#url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY_EXTENDED&symbol=IBM&interval=60min&slice=year1month3&adjusted=false&apikey={av_apikey}\"\n",
    "\"\"\"\n",
    "\n",
    "#//******************************************************************************\n",
    "#//*** Builds the URL request based on the symbol and type of data requested.\n",
    "#//*** Initially, this does the daily numbers.\n",
    "#//*** Can easily be scaled up to add many different URL request types\n",
    "#//******************************************************************************\n",
    "def build_url(input_action,input_symbol):\n",
    "    #//*** Valid Actions:\n",
    "    #//*******  Daily: Gets the historical daily closing price for up to 20 years\n",
    "    \n",
    "    if input_action == 'daily':\n",
    "        action = \"TIME_SERIES_DAILY\"\n",
    "        out = \"\"\n",
    "        out += f'https://www.alphavantage.co/query?'\n",
    "        out += f'function={action}'\n",
    "        out += f'&symbol={symbol}'\n",
    "        out += f'&outputsize=full'\n",
    "        out += f'&apikey={av_apikey}'\n",
    "        \n",
    "        return url\n",
    "    \n",
    "    print(f\"Invalid Action: {input_action}\")\n",
    "    print(f\"No URL Returned, PLease try again\")\n",
    "    return None\n",
    "    \n",
    "\n",
    "\n",
    "action = 'daily'\n",
    "\n",
    "\n",
    "\n",
    "for symbol in symbols:\n",
    "    \n",
    "    #//*** Build the Url Request for each symbol\n",
    "    print(f\"Building URL: {symbol}\")\n",
    "    url = build_url(action,symbol)\n",
    "    #//*** Verify we built a proper url\n",
    "    if url != None:\n",
    "        \n",
    "        print(\"Downloading....\")\n",
    "        #//*** Download the data for each Symbol\n",
    "        r = requests.get(url)\n",
    "        \n",
    "        #//*** Convert raw string to dictionary for processing \n",
    "        data = r.json()\n",
    "\n",
    "        #//*** Output Dictionary\n",
    "        out_dict = {}\n",
    "        print(\"Processing....\")\n",
    "        #//*** Process Data into the out_dict\n",
    "        for date in data[data_key]:\n",
    "            #//*** Build out_dict (output_dictionary) keys \n",
    "            if len(out_dict.keys()) == 0:\n",
    "                out_dict['date'] = []\n",
    "                out_dict['symbol'] = []\n",
    "\n",
    "                #//*** Get this dictionary for the first row. Use the key values, but strip the first 3 characters which are numeric\n",
    "                for key in data[data_key][date].keys():\n",
    "                    out_dict[key[3:]] = []\n",
    "\n",
    "            #//*** Add Date to out_dict\n",
    "            out_dict['date'].append(date)\n",
    "\n",
    "            #//*** Add Symbol to out_dict\n",
    "            out_dict['symbol'].append(symbol)\n",
    "\n",
    "            #//*** Loop through the daily values and append to the out_dict\n",
    "            for key,value in data[data_key][date].items():\n",
    "\n",
    "                #//*** Trim first 3 characters off key and append to the appropriate dictionary list\n",
    "                out_dict[key[3:]].append(value)\n",
    "\n",
    "        print(\"Building Dataframe\")\n",
    "        out_df = pd.DataFrame()\n",
    "        #//*** Convert the Dictionary to a Dataframe\n",
    "        #//*** Each Key is a column, the data is the list\n",
    "        for key,value in out_dict.items():\n",
    "            out_df[key] = value\n",
    "\n",
    "        #//*** Generic Filename - Placeholder\n",
    "        output_filename = f\".\\\\stocks\\\\{symbol}_need_a_better_name.csv.zip\"\n",
    "        \n",
    "        #//*** Build filename based on action type\n",
    "        if action == 'daily':\n",
    "            output_filename = f\".\\\\stocks\\\\{symbol}_daily.csv.zip\"\n",
    "        \n",
    "        print(f\"Writing dataframe to File: {output_filename}\")\n",
    "        out_df.to_csv(output_filename,compression=\"zip\",index=False)    \n",
    "\n",
    "    else:\n",
    "        print(\"We've got an url problem Skipping\")\n",
    "    \n",
    "    #//*** Wait 20 seconds so we don't hammer the API\n",
    "    #//*** Max is 5 calls / minute & 500 /day\n",
    "    \n",
    "    print(\"Waiting 20 Seconds\")\n",
    "    time.sleep(20)\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#//**** INTRA day EXTENDED provides stock data at intervals of 1min, 5min, 15min, 30min, 60min,\\n#//**** Each query provides one month at a time\\n\\nimport csv\\nCSV_URL = f'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY_EXTENDED&symbol={symbol}&interval=15min&slice=year1month1&apikey={av_apikey}'\\n\\nwith requests.Session() as s:\\n    download = s.get(url)\\n    decoded_content = download.content.decode('utf-8')\\n    cr = csv.reader(decoded_content.splitlines(), delimiter=',')\\n    my_list = list(cr)\\n    for row in my_list:\\n        print(row)\\n\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#//**** INTRA day EXTENDED provides stock data at intervals of 1min, 5min, 15min, 30min, 60min,\n",
    "#//**** Each query provides one month at a time\n",
    "\n",
    "import csv\n",
    "CSV_URL = f'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY_EXTENDED&symbol={symbol}&interval=15min&slice=year1month1&apikey={av_apikey}'\n",
    "\n",
    "with requests.Session() as s:\n",
    "    download = s.get(url)\n",
    "    decoded_content = download.content.decode('utf-8')\n",
    "    cr = csv.reader(decoded_content.splitlines(), delimiter=',')\n",
    "    my_list = list(cr)\n",
    "    for row in my_list:\n",
    "        print(row)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
