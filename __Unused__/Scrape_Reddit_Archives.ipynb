{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Level 1: Data Acquisition ##\n",
    "This code downloads 500,000 comments from each of 16 different meme stock related subreddits.\n",
    "\n",
    "Uses Pushshift.io published Python library for API access. The API breaks all requests into separate 1000 comment request chunks. There are many duplicates returned in each search sometimes as high as 20%. This must be due to the distributed nature of the database the API is pulling from. To de-duplicate efficiently, comments are hashed with a SHA-256 algorithm in order to deduplicate.\n",
    "\n",
    "Comments for each stock ticker are stored in separate CSV files to be used in Pipeline Level 2.\n",
    "\n",
    "The process of downloading all the comments took around 14 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/swlh/how-to-scrape-large-amounts-of-reddit-data-using-pushshift-1d33bde9286"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1y/5m3skjbn33ggq1vw0_jc_wcc0000gn/T/ipykernel_72600/1893541391.py:14: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1666852281\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# //*** Imports and Load Data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import platform\n",
    "import time\n",
    "from pmaw import PushshiftAPI\n",
    "\n",
    "\n",
    "#//*** Use the whole window in the IPYNB editor\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#//*** Maximize columns and rows displayed by pandas\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "api = PushshiftAPI()\n",
    "\n",
    "import datetime as dt\n",
    "before = int(dt.datetime(2021,6,26,0,0).timestamp())\n",
    "after = int(dt.datetime(2020,12,1,0,0).timestamp())\n",
    "\n",
    "print(int(time.time()))\n",
    "\n",
    "subreddits = [\"wallstreetbets\", \"stocks\", \"wallstreetbetsOGs\", \"spacs\", \"investing\", \"pennystocks\", \"stockmarket\", \"options\", \"robinhoodpennystocks\", \"wallstreetbetsnew\", \"smallstreetbets\"]\n",
    "subreddits = [\"stocks\", \"wallstreetbetsOGs\", \"spacs\", \"investing\", \"pennystocks\", \"stockmarket\", \"options\", \"robinhoodpennystocks\", \"wallstreetbetsnew\", \"smallstreetbets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    }
   ],
   "source": [
    "from pmaw import PushshiftAPI\n",
    "\n",
    "api = PushshiftAPI()\n",
    "\n",
    "comments = api.search_comments(subreddit=subreddits, limit=100, before=before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Update a target subreddit with new data\n",
    "#//*** subreddit = subbreddit name\n",
    "#//*** Method = 'before' or 'after' indicating if the records to be retrieved are before or after the target_utc. Defaults to After\n",
    "#//*** limit is the number or records to grab\n",
    "def update_subreddit(subreddit,method,limit):\n",
    "    import time\n",
    "    filename = f\"..\\\\data\\\\{subreddit}_comments.csv.zip\"\n",
    "\n",
    "    #//*** Convert Path to Mac formatting if needed\n",
    "    if platform.system() == 'Darwin':\n",
    "        filename = filename.replace(\"\\\\\",\"/\")\n",
    "    \n",
    "    #//*** Check if File exists\n",
    "    if os.path.isfile(filename):\n",
    "        print (\"Update Existing File\")\n",
    "\n",
    "        print(f\"Reading csv: {filename}\")\n",
    "        start_time = time.time()\n",
    "        update_df = pd.read_csv(filename, compression = 'zip')\n",
    "\n",
    "        print(f\"csv loaded: {round(time.time()-start_time,2)}s\")\n",
    "\n",
    "        print(f\"csv Record count: {len(update_df)}\")\n",
    "\n",
    "        #//*** If before, get the largest (latest) utc\n",
    "        if method == 'before':\n",
    "            #//*** Get the Before utc from the stored csv\n",
    "            before = update_df['created_utc'].min()\n",
    "\n",
    "            print(f\"Getting {limit} records before {before} utc\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            #//*** Download comments\n",
    "            comments = api.search_comments(subreddit=subreddit, limit=limit, before=before)\n",
    "\n",
    "            print(f\"Download Time: {round(time.time()-start_time,2)}s\")\n",
    "\n",
    "        elif method == 'after':\n",
    "            after = update_df['created_utc'].max() \n",
    "\n",
    "            print(f\"Getting {limit} records after {after} utc\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            #//*** Download comments\n",
    "            comments = api.search_comments(subreddit=subreddit, limit=limit, after=after)\n",
    "            print(f\"Download Time: {round(time.time()-start_time,2)}s\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Method needs to be 'before' or 'after': [{method}] is invalid\")\n",
    "            return\n",
    "    else:\n",
    "        #//*** This is a new file, download 100k records starting right now\n",
    "        after = int(time.time())\n",
    "        before = int(time.time())\n",
    "        \n",
    "        limit = 1000000\n",
    "        limit = 500000\n",
    "        \n",
    "        print(f\"Getting {limit} records before {after} utc\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        update_df = pd.DataFrame()\n",
    "\n",
    "        #//*** Download comments\n",
    "        comments = api.search_comments(subreddit=subreddit, limit=100, before=before)\n",
    "\n",
    "        print(f\"Download Time: {round(time.time()-start_time,2)}s\")\n",
    "    \n",
    "\n",
    "    \n",
    "    #//***************************************************************************\n",
    "    #//*** Download Complete\n",
    "    #//***************************************************************************\n",
    "    \n",
    "    #//*** Convert comments to Dataframe\n",
    "    raw_df = pd.DataFrame(comments)\n",
    "\n",
    "    if len(raw_df) == 0:\n",
    "        \n",
    "        print(f\"{subreddit} {method} {limit} - No Records Returned\")\n",
    "        return\n",
    "    \n",
    "    #//*** Columns to keep\n",
    "    keep_cols = [\"score\",\"total_awards_received\",\"created_utc\",\"is_submitter\",\"author_fullname\",\"body\",\"id\",\"link_id\",\"parent_id\",\"stickied\",\"permalink\",\"retrieved_on\",\"subreddit\",\"subreddit_id\"]\n",
    "    \n",
    "    #//*** Not all columns appear. This usually happens with small samples used for testing.\n",
    "    #//*** Only use the keep_cols that are actually in the sample. The missing columns will be added during concat later\n",
    "    actual_cols = []\n",
    "    \n",
    "    #//*** Loop through each column we want to keep\n",
    "    for col in keep_cols:\n",
    "        #//*** Add col to actual_cols if it exists\n",
    "        if col in raw_df.columns:\n",
    "            actual_cols.append(col)\n",
    "\n",
    "    #//*** Keep the important columns\n",
    "    raw_df = raw_df[actual_cols]\n",
    "    \n",
    "    print(f\"Checking For Duplicates - Length Before: {len(raw_df)}\")\n",
    "    \n",
    "    #//*** Hash the body, will use to check for duplicates\n",
    "    #//*** Hash the body using sha-256\n",
    "    #Sha256: Reference https://www.pythonpool.com/python-sha256/\n",
    "\n",
    "    raw_df['hash'] = raw_df['body'].apply(lambda x:hashlib.sha256(x.encode()).hexdigest())\n",
    "\n",
    "\n",
    "    # dropping Duplicates First. No sense in processing these\n",
    "    raw_df.drop_duplicates(subset =\"hash\",keep = False, inplace = True)\n",
    "    \n",
    "    print(f\"Checking For Duplicates - Length After: {len(raw_df)}\")\n",
    "\n",
    "    #print(\"Begin Cleaning\")\n",
    "\n",
    "    #//*** Clean text, tokenize and remove stop words\n",
    "    #raw_df['clean'] = remove_stop_words(tokenize_series(mr_clean_text(raw_df['body'],{\"remove_empty\":False})))\n",
    "    \n",
    "    #//*** encode the comments\n",
    "    #//*** Breaking this out into a separate function for readability and possible future flexibility\n",
    "    #raw_df = encode_comments(raw_df)\n",
    "    \n",
    "    #//*** Combining existing dataframe with raw_df\n",
    "    update_df = pd.concat([update_df,raw_df])\n",
    "    print(f\"Combined Dataframe Size:{len(update_df)}\")\n",
    "\n",
    "    # Check for Duplicates. \n",
    "    update_df.drop_duplicates(subset =\"hash\",keep = False, inplace = True)\n",
    "    print(f\"Dropping Duplicates - New Size:{len(update_df)}\")\n",
    "\n",
    "    #print(\"Replace NaN with Zeros\")\n",
    "    update_df = update_df.fillna(0)\n",
    "    \n",
    "    #//*** Sort the Dataframe by UTC date. This keeps the time series chronological. \n",
    "    #//*** No need to reindex, since index will be ignored at csv read/write\n",
    "    update_df = update_df.sort_values('created_utc')\n",
    "\n",
    "    #//*** Convert is Submitter,Stickied field to Boolean.\n",
    "    #//*** Some early values are Integers and Strings\n",
    "    update_df['is_submitter'] = update_df['is_submitter'].astype('bool')\n",
    "    update_df['stickied'] = update_df['stickied'].astype('bool')\n",
    "    update_df['author_fullname'] = update_df['author_fullname'].astype('str')\n",
    "    \n",
    "    print(f\"Writing {filename}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    update_df.to_csv(filename,compression=\"zip\",index=False)    \n",
    "    \n",
    "    print(f\"File Written: {round(time.time()-start_time,2)}s\")\n",
    "    \n",
    "    print(f\"update_subreddit() Complete: {len(update_df)} records\")\n",
    "    \n",
    "    del update_df\n",
    "    del raw_df\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating: stocks\n",
      "Update Existing File\n",
      "Reading csv: ../data/stocks_comments.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1y/5m3skjbn33ggq1vw0_jc_wcc0000gn/T/ipykernel_72600/403067405.py:19: DtypeWarning: Columns (0,1,2,3,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  update_df = pd.read_csv(filename, compression = 'zip')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv loaded: 2.41s\n",
      "csv Record count: 466922\n",
      "Updating: wallstreetbetsOGs\n",
      "Update Existing File\n",
      "Reading csv: ../data/wallstreetbetsOGs_comments.csv.zip\n",
      "csv loaded: 1.95s\n",
      "csv Record count: 451021\n",
      "Getting 100 records before 1611978410 utc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Time: 0.26s\n",
      "wallstreetbetsOGs before 100 - No Records Returned\n",
      "Updating: spacs\n",
      "Update Existing File\n",
      "Reading csv: ../data/spacs_comments.csv.zip\n",
      "csv loaded: 2.13s\n",
      "csv Record count: 449120\n",
      "Getting 100 records before 1590178346 utc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Time: 9.85s\n",
      "Checking For Duplicates - Length Before: 100\n",
      "Checking For Duplicates - Length After: 100\n",
      "Combined Dataframe Size:449220\n",
      "Dropping Duplicates - New Size:449220\n",
      "Writing ../data/spacs_comments.csv.zip\n",
      "File Written: 6.94s\n",
      "update_subreddit() Complete: 449220 records\n",
      "Updating: investing\n",
      "Update Existing File\n",
      "Reading csv: ../data/investing_comments.csv.zip\n",
      "csv loaded: 2.38s\n",
      "csv Record count: 421647\n",
      "Getting 100 records before 1565372215 utc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Time: 2.16s\n",
      "Checking For Duplicates - Length Before: 100\n",
      "Checking For Duplicates - Length After: 98\n",
      "Combined Dataframe Size:421745\n",
      "Dropping Duplicates - New Size:421739\n",
      "Writing ../data/investing_comments.csv.zip\n",
      "File Written: 8.6s\n",
      "update_subreddit() Complete: 421739 records\n",
      "Updating: pennystocks\n",
      "Update Existing File\n",
      "Reading csv: ../data/pennystocks_comments.csv.zip\n",
      "csv loaded: 1.84s\n",
      "csv Record count: 412154\n",
      "Getting 100 records before 1593804308 utc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Time: 1.32s\n",
      "Checking For Duplicates - Length Before: 100\n",
      "Checking For Duplicates - Length After: 96\n",
      "Combined Dataframe Size:412250\n",
      "Dropping Duplicates - New Size:412248\n",
      "Writing ../data/pennystocks_comments.csv.zip\n",
      "File Written: 6.32s\n",
      "update_subreddit() Complete: 412248 records\n",
      "Updating: stockmarket\n",
      "Update Existing File\n",
      "Reading csv: ../data/stockmarket_comments.csv.zip\n",
      "csv loaded: 2.22s\n",
      "csv Record count: 456443\n",
      "Getting 100 records before 1530834525 utc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Time: 1.12s\n",
      "Checking For Duplicates - Length Before: 100\n",
      "Checking For Duplicates - Length After: 92\n",
      "Combined Dataframe Size:456535\n",
      "Dropping Duplicates - New Size:456535\n",
      "Writing ../data/stockmarket_comments.csv.zip\n",
      "File Written: 7.87s\n",
      "update_subreddit() Complete: 456535 records\n",
      "Updating: options\n",
      "Update Existing File\n",
      "Reading csv: ../data/options_comments.csv.zip\n",
      "csv loaded: 2.36s\n",
      "csv Record count: 451565\n",
      "Getting 100 records before 1562274981 utc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Time: 1.21s\n",
      "Checking For Duplicates - Length Before: 100\n",
      "Checking For Duplicates - Length After: 100\n",
      "Combined Dataframe Size:451665\n",
      "Dropping Duplicates - New Size:451665\n",
      "Writing ../data/options_comments.csv.zip\n",
      "File Written: 8.62s\n",
      "update_subreddit() Complete: 451665 records\n",
      "Updating: robinhoodpennystocks\n",
      "Update Existing File\n",
      "Reading csv: ../data/robinhoodpennystocks_comments.csv.zip\n",
      "csv loaded: 1.84s\n",
      "csv Record count: 428037\n",
      "Getting 100 records before 1531333645 utc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Time: 7.31s\n",
      "Checking For Duplicates - Length Before: 100\n",
      "Checking For Duplicates - Length After: 98\n",
      "Combined Dataframe Size:428135\n",
      "Dropping Duplicates - New Size:428135\n",
      "Writing ../data/robinhoodpennystocks_comments.csv.zip\n",
      "File Written: 6.11s\n",
      "update_subreddit() Complete: 428135 records\n",
      "Updating: wallstreetbetsnew\n",
      "Update Existing File\n",
      "Reading csv: ../data/wallstreetbetsnew_comments.csv.zip\n",
      "csv loaded: 1.56s\n",
      "csv Record count: 381419\n",
      "Getting 100 records before 1594481539 utc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Time: 8.85s\n",
      "Checking For Duplicates - Length Before: 100\n",
      "Checking For Duplicates - Length After: 98\n",
      "Combined Dataframe Size:381517\n",
      "Dropping Duplicates - New Size:381515\n",
      "Writing ../data/wallstreetbetsnew_comments.csv.zip\n",
      "File Written: 5.1s\n",
      "update_subreddit() Complete: 381515 records\n",
      "Updating: smallstreetbets\n",
      "Update Existing File\n",
      "Reading csv: ../data/smallstreetbets_comments.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv loaded: 0.64s\n",
      "csv Record count: 138856\n",
      "Getting 100 records before 1575514800 utc\n",
      "Download Time: 0.19s\n",
      "smallstreetbets before 100 - No Records Returned\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#//*** Add 100k Comments to wallstreetbets\n",
    "#update_subreddit(\"wallstreetbets\",\"after\",100)\n",
    "\n",
    "#update_subreddit(\"stocks\",\"after\",100)\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    print(f\"Updating: {subreddit}\")\n",
    "    try:\n",
    "        update_subreddit(subreddit,\"before\",100)\n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(raw_df['hash'].unique()))\n",
    "#print(len(raw_df.tail()))\n",
    "#raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/muduo/GitHub/nlp_trader\n"
     ]
    }
   ],
   "source": [
    "cd nlp_trader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>is_submitter</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>body</th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>stickied</th>\n",
       "      <th>permalink</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1334162803</td>\n",
       "      <td>False</td>\n",
       "      <td>t2_59t5b</td>\n",
       "      <td>This is a fantastic idea! I'll toss mine up in...</td>\n",
       "      <td>c4b0pvu</td>\n",
       "      <td>t3_s4jw1</td>\n",
       "      <td>t3_s4jw1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1.428700e+09</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>6827bc9e2385d87ecf7e53c54baab15186a20b47d0dde0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score  total_awards_received  created_utc  is_submitter author_fullname  \\\n",
       "0      2                    0.0   1334162803         False        t2_59t5b   \n",
       "\n",
       "                                                body       id   link_id  \\\n",
       "0  This is a fantastic idea! I'll toss mine up in...  c4b0pvu  t3_s4jw1   \n",
       "\n",
       "  parent_id  stickied permalink  retrieved_on       subreddit subreddit_id  \\\n",
       "0  t3_s4jw1     False         0  1.428700e+09  wallstreetbets     t5_2th52   \n",
       "\n",
       "                                                hash  \n",
       "0  6827bc9e2385d87ecf7e53c54baab15186a20b47d0dde0...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# //*** Reference to manually read & write to Data Frame\n",
    "# filename = f\"data/wallstreetbets_comments.csv.zip\"\n",
    "# update_df = pd.read_csv(filename, compression = 'zip')\n",
    "# print(len(update_df))\n",
    "update_df[0:1]\n",
    "# update_df\n",
    "# filename = f\".\\\\data\\\\wallstreetbets_comments_comments.csv\"\n",
    "# update_df.to_csv(filename, compression = 'zip',index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_df.to_csv(\"reddit_comments.csv.zip\",compression=\"zip\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #//*** Push shift scraper reference\n",
    "# \"\"\"\n",
    "# #//*** Download the First 100,000 Comments from reddit pushsift\n",
    "# subreddit=\"wallstreetbets\"\n",
    "# limit=100\n",
    "# #comments = api.search_comments(subreddit=subreddit, limit=limit, before=before after=after)\n",
    "# comments = api.search_comments(subreddit=subreddit, limit=limit, after=after)\n",
    "# print(f'Retrieved {len(comments)} comments from Pushshift')\n",
    "\n",
    "# #//*** Convert comments to Dataframe\n",
    "# comments_df = pd.DataFrame(comments)\n",
    "\n",
    "# #//*** Save DataFrame to a file for processing\n",
    "# comments_df.to_csv(f\"{subreddit}_raw_comments.csv.zip\",compression=\"zip\",index=False)\n",
    "# \"\"\"\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Reference to create Stock ticker count matrix\n",
    "\"\"\"\n",
    "\n",
    "#//*** Build list of ticker symbols from NYSE and NASDAQ\n",
    "#//*** Reads from Excel file.\n",
    "#//*** Gets the Symbol column, and converts to lower case, \n",
    "nyse = pd.read_csv(\"NYSE_20210625.csv\",header=None)[0].str.lower()\n",
    "nasdaq = pd.read_csv(\"NASDAQ_20210625.csv\",header=None)[0].str.lower()\n",
    "\n",
    "#//*** Removes symbols with 1 and 2 character listings\n",
    "nyse = list(nyse[nyse.apply(lambda x: len(x)>2) ])\n",
    "nasdaq = list(nasdaq[nasdaq.apply(lambda x: len(x)>2) ])\n",
    "\n",
    "symbols = nyse + nasdaq\n",
    "\n",
    "#//*** Count each Stock mention add it to a dictionary of lists. Each list is filled with 0s. The Specific row index is updated with the relevant count. \n",
    "#//*** This Generates a word count matrix\n",
    "stock_dict = {}\n",
    "\n",
    "#//*** Keep Track of Rows\n",
    "index = 0\n",
    "\n",
    "for row in raw_df.iterrows():\n",
    "    \n",
    "    #//*** Get the cleaned body text\n",
    "    body = row[1]['clean']\n",
    "    \n",
    "    #//*** For Each Stock Symbol\n",
    "    for stock in symbols:\n",
    "        \n",
    "        #//*** Check if Stock exists in Body\n",
    "        if stock in body:\n",
    "            \n",
    "            #//*** Reset the stock counter\n",
    "            count = 0\n",
    "            \n",
    "            #//*** Loop through body and county ticker mentions\n",
    "            for word in body:\n",
    "                #//*** If word found increment count\n",
    "                if stock == word:\n",
    "                    count += 1\n",
    "                    \n",
    "            #//*** Check if symbol is in stock_dict\n",
    "            if stock not in stock_dict.keys():    \n",
    "\n",
    "                #//*** If not, then build it\n",
    "                stock_dict[stock] = np.zeros(len(raw_df))\n",
    "            \n",
    "            #//*** Update the stock value at the \n",
    "            stock_dict[stock][index] = count\n",
    "\n",
    "    #//*** Increment Index to keep with row index\n",
    "    index +=1   \n",
    "\n",
    "#//*** Loop through the dictionary key and lists\n",
    "for col,values in stock_dict.items():\n",
    "    \n",
    "    #//*** Add each key (which is a stock ticker symbol) as a column using the list of ticker counts for Data\n",
    "    raw_df[col] = values.astype('int')\n",
    "    \n",
    "\"\"\"\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('sentinvest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "715913a423b20e2c0e9b75ed171450f8771ad7a22d7d59f856f5b3263351e86f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
